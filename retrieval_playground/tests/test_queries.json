[
  {
    "user_input": "How does MC3G improve upon existing counterfactual explanation methods, particularly concerning cost computation and causal dependencies?",
    "reference_context": "Existing counterfactual generation methods face key limitations: 1) Not model agnostic: Many approaches require direct access to model parameters, making them impractical for proprietary systems. 2) Ignore causal dependencies: Traditional methods assume features can be altered independently, leading to unrealistic counterfactuals (e.g., arbitrarily increasing credit score without addressing underlying financial history). 3) Inefficient cost computation: Most frameworks treat all feature changes equally, failing to distinguish between user-initiated changes and automatic adjustments due to causal relationships.\n\nTo address these limitations, we propose Model-Agnostic Causally Constrained Counterfactual Generation (MC3G), a novel framework that enhances the realism, interpretability, and usability of counterfactual explanations while maintaining model secrecy. Our key contributions are as follows: 1) Model-agnostic framework: Instead of accessing the black-box model directly, MC3G first approximates it using an explainable rule-based model (e.g., FOLD-SE). This allows MC3G to work across different machine learning models without compromising their proprietary nature. 2) Refined cost computation: MC3G separates user-initiated interventions from automatic feature adjustments caused by causal dependencies. This ensures a more accurate and fair representation of effort required to achieve a counterfactual outcome. 3) Improved actionability and interpretability: By enforcing causal constraints, MC3G generates realistic counterfactuals that align with real-world constraints (e.g., increasing credit score realistically through debt reduction). This makes the generated explanations more useful and actionable for individuals seeking recourse.",
    "reference": "MC3G addresses limitations in existing methods by being model-agnostic, using an explainable rule-based surrogate model to approximate any black-box model.  It also incorporates causal dependencies, generating more realistic counterfactuals by avoiding unrealistic independent feature alterations.  Finally, MC3G refines cost computation by only considering user-initiated changes, excluding automatically adjusted features due to causal relationships, providing a fairer representation of the effort needed for a favorable outcome.",
    "source_file": "Analytics_2025_MC3G__Model_Agnostic_Causally_Constrained_Counterfactual_Generation.pdf"
  },
  {
    "user_input": "What limitations of the Copas-Jackson bound did the authors identify, and how did they address these limitations in their proposed method?",
    "reference_context": "The nonparametric class of the selection models in the C-J bound is restrictive and only covers parametric selection models monotonic to the standard errors of outcomes. The novelty of this paper is to develop a method that constructs worst-case bounds over a general class of selection models weakening the assumption in the C-J bound. We propose an efficient numerical method to obtain an approximate worst-case bound via tractable nonlinear programming with linear constraints. We substantiate the effectiveness of the proposed bound with extensive simulation studies and show its applicability with two real-world meta-analyses.  We showed that the assumption A 0 is not necessarily satisfied by some representative parametric selection models. Since selection models monotone with respect to t -statistics are a very natural class of selective publication mechanism, it is attractive to evaluate the worst-case bounds over a class of the selection models monotone with respect to t -statistics.  Thus, obtaining the maximum and the minimum of b over A 1 is a relaxation problem for the original problem. They provide upper and lower bounds for the bounds over the class of selection models monotone with respect to t -statistics although they may not be tight.",
    "reference": "The authors found that the Copas-Jackson (C-J) bound's nonparametric class of selection models is restrictive, only encompassing parametric models monotonic with respect to standard errors.  Their proposed method addresses this by constructing worst-case bounds over a more general class of selection models, relaxing the C-J bound's assumptions.  They achieve this through a numerical method using nonlinear programming with linear constraints.  Additionally, they acknowledge that their relaxed assumption, while covering t-statistic type selection models, might not yield the sharpest bounds.",
    "source_file": "Statistics_2025_Copas-Jackson-type_bounds_for_publication_bias_over_a_general_class_of___selecti.pdf"
  },
  {
    "user_input": "What is the key innovation of the proposed Riemannian change point detection method, and how does it improve upon previous approaches?",
    "reference_context": "In [14, 15], the authors consider only the Karcher mean estimator defined in (2). In this work, we extend this strategy by viewing the Karcher mean estimator as a special instance of the robust centroid estimator in (3) corresponding to the choice of A = ∞ in (4). We further compare this adaptive centroid estimator with another special case of the robust centroid estimator obtained by setting A ∈ (0 , ∞ ) in (4), which provides enhanced robustness in the presence of outliers - in our context, the post-change samples.  The primary contribution of this paper is the introduction of robust centroid estimation into Riemannian CPD. We specially explore two special cases of the Huber centroid, one prioritizing adaptability and the other emphasizing robustness. The comparison between these two centroids forms the basis for our test statistic, which notably reduces the dependence on the convergence of the estimation algorithm. Additionally, we propose an online estimation method for the Huber centroid using a Riemannian stochastic optimization algorithm.",
    "reference": "The key innovation is the introduction of robust centroid estimation, specifically using the Huber function, into Riemannian change point detection (CPD).  Previous methods relied on the Karcher mean, which is sensitive to outliers. This new method compares two centroid estimates: a Karcher mean (adaptive) and a Huber-based centroid (robust). This comparison creates a test statistic less sensitive to the estimation algorithm's convergence and more robust to outliers caused by change points, improving upon previous approaches that only used the Karcher mean.",
    "source_file": "Machine_Learning_2025_Riemannian_Change_Point_Detection_on_Manifolds_with_Robust_Centroid___Estimation.pdf"
  },
  {
    "user_input": "What are the key challenges in adapting existing open-vocabulary semantic segmentation (OVSS) frameworks, designed for natural images, to remote sensing images, and how does SegEarth-OV address these challenges?",
    "reference_context": "However, the adaptation of generic OVSS frameworks, especially those leveraging pre-trained Vision-Language Models (VLMs) such as CLIP [6], to remote sensing images encounters substantial challenges. Remote sensing images inherently exhibit multi-scale characteristics, encompassing objects spanning orders of magnitude, from vast land cover to fine structures such as small buildings or vehicles. The direct application of VLMs, particularly those employing aggressive downsampling strategies ( e.g. , 1/16th of the original image resolution for ViT-based CLIP), inevitably leads to irreversible loss of fine-grained spatial details. As shown in Fig. 1 (a)(b), the resulting distorted target shapes and imprecise boundaries compromise the segmentation accuracy, thus undermining the effectiveness of dense prediction. Moreover, VLMs like CLIP are pre-trained on largescale image-text pairs of natural images, where their [CLS] tokens encode 'global bias' [7]-[10]. While this bias aids image-level classification, it inadvertently 'leaks' into local patch features, negatively impacting the precise pixel-level understanding, which is essential for semantic segmentation. To address these critical challenges, we introduced SegEarth-OV [11], the first framework designed for annotation-free OVSS of remote sensing images. To tackle the issues of distorted shapes and imprecise boundaries, we propose SimFeatUp, a universal upsampler that meticulously reconstructs high-resolution spatial details from low-resolution VLM features through a parameterized Joint Bilateral Upsampling (JBU) mechanism, coupled with a Content Retention Network (CRN) that ensures content consistency. Crucially, after a one-time general training, SimFeatUp operates without any task-specific post-training [12], [13]. To enhance local semantic fidelity, we present Global Bias Alleviation, a simple yet effective subtraction operation that mitigates the global bias from VLM patch features, enabling more precise pixel-level predictions.",
    "reference": "Existing OVSS frameworks struggle when applied to remote sensing images due to multi-scale characteristics leading to loss of fine-grained details from downsampling and inherent global biases in VLMs like CLIP that negatively impact pixel-level understanding. SegEarth-OV addresses these by introducing SimFeatUp, a universal upsampler restoring high-resolution spatial details, and Global Bias Alleviation, mitigating the global bias from VLM patch features to enhance local semantic fidelity.",
    "source_file": "Computer_Vision_2025_Annotation-Free_Open-Vocabulary_Segmentation_for_Remote-Sensing_Images.pdf"
  },
  {
    "user_input": "What are the key decidable restrictions on the SSM satisfiability problem identified in the paper, and what are their corresponding complexity bounds?",
    "reference_context": "Motivated by practical settings, we identify two natural restrictions under which ssmSAT becomes decidable and establish corresponding complexity bounds. First, for SSM with bounded context length, ssmSAT is NP-complete when the input length is given in unary and in NEXPTIME (and PSPACE-hard) when the input length is given in binary. Second, for quantised SSM operating over fixed-width arithmetic, ssmSAT is PSPACE-complete resp. in EXPSPACE depending on the bit-width encoding. While these results hold for diagonal gated SSM we also establish complexity bounds for time-invariant SSM. Our results establish a first complexity landscape for formal reasoning in SSM and highlight fundamental limits and opportunities for the verification of SSM-based language models.",
    "reference": "The paper identifies two key decidable restrictions: (1) Bounded context length, resulting in NP-completeness with unary input length and NEXPTIME (PSPACE-hard) with binary input length; and (2) Quantized SSM with fixed-width arithmetic, leading to PSPACE-completeness or EXPSPACE membership depending on the bit-width encoding.  These complexities apply to diagonal gated SSM, with additional bounds provided for time-invariant SSM.",
    "source_file": "Generative_AI_2025_The_Computational_Complexity_of_Satisfiability_in_State_Space_Models.pdf"
  }
]