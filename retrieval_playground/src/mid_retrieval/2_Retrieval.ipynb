{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Methods Tutorial\n",
    "\n",
    "This notebook demonstrates various retrieval methods for RAG systems using vector databases. Each method serves different use cases and has specific advantages.\n",
    "\n",
    "## ğŸ“š Overview of Methods\n",
    "\n",
    "1. **Basic Similarity Search** - Standard semantic search\n",
    "2. **MMR (Maximal Marginal Relevance)** - Balance relevance and diversity\n",
    "3. **Score Thresholding** - Quality-based filtering\n",
    "4. **Adaptive/Dynamic Retrieval** - Flexible result counts\n",
    "5. **Metadata Filtering** - Context-aware search\n",
    "6. **Document Chunk Linking** - Multi-document retrieval\n",
    "7. **LLM-Guided Filtering** - Intelligent pre-filtering\n",
    "8. **Reranking** - Reorder results for higher precision  \n",
    "9. **Hybrid Retrieval** - Combine keyword and semantic search  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, Filter, FieldCondition, MatchValue, MatchText\n",
    "from retrieval_playground.utils import config, constants\n",
    "from retrieval_playground.utils.model_manager import model_manager\n",
    "from retrieval_playground.src.pre_retrieval.chunking_strategies import ChunkingStrategy\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import Dict, List, Any\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample query: How does MC3G improve upon existing counterfactual explanation methods, particularly concerning cost computation and causal dependencies?\n"
     ]
    }
   ],
   "source": [
    "# Load test queries\n",
    "def load_test_queries() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load test queries from JSON file.\"\"\"\n",
    "    queries_path = config.TESTS_DIR / \"test_queries.json\"\n",
    "    with open(queries_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "test_queries = load_test_queries()\n",
    "sample_query = test_queries[0][\"user_input\"]\n",
    "print(f\"Sample query: {sample_query}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-10 17:33:09.287 INFO model_manager - _initialize_models: ğŸ”„ ModelManager: Initializing shared AI models...\n",
      "2025-09-10 17:33:17.693 INFO model_manager - _initialize_models: âœ… ModelManager: Shared AI models initialized successfully\n",
      "âœ… Vector store initialized\n"
     ]
    }
   ],
   "source": [
    "# Setup vector database\n",
    "strategy = ChunkingStrategy.UNSTRUCTURED\n",
    "qdrant_path = config.QDRANT_DIR / strategy.value\n",
    "qdrant_client = QdrantClient(path=str(qdrant_path))\n",
    "embeddings = model_manager.get_embeddings()\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=strategy.value,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(\"âœ… Vector store initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters\n",
    "TOP_K = 3\n",
    "SCORE_THRESHOLD = 0.35\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Basic Similarity Search ğŸ¯\n",
    "\n",
    "**ğŸ” What it is:**  Standard cosine similarity between query and document embeddings.\n",
    "\n",
    "**ğŸ¯ When to use:**\n",
    "- Default choice for most RAG applications\n",
    "- When you need the most semantically similar content\n",
    "- Simple, fast, and reliable\n",
    "\n",
    "**ğŸŒŸ Pros:** Fast, simple, works well for most cases  \n",
    "**âš ï¸ Cons:** May return very similar/duplicate content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Similarity Search Results:\n",
      "1. Score: 0.762 | Source: Statistics_2025_Estimating_the_average_treatment_effect_in_cluster-randomized_trials___with_misc.pdf | ChunkID: 0fd71936-03f5-497e-ac0d-e63f946e558b \n",
      "   Preview: Statistical Methods in Medical Research, 22(3):278...\n",
      "\n",
      "2. Score: 0.735 | Source: Generative_AI_2025_Aligning_the_Evaluation_of_Probabilistic_Predictions_with_Downstream___Value.pdf | ChunkID: 8fa0aa27-cbc9-40e7-afec-76bc574d3f5d \n",
      "   Preview: âˆ— Corresponding Author. Email: novin.shahroudi@ut....\n",
      "\n",
      "3. Score: 0.724 | Source: Analytics_2025_A_Human-In-The-Loop_Approach_for_Improving_Fairness_in_Predictive___Business_Pro.pdf | ChunkID: 1661a222-0d47-4013-85d2-67864e5a9651 \n",
      "   Preview: MAblation Study Results\n",
      "\n",
      "Fig.6: Results of our abl...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic similarity search with scores\n",
    "context_docs_with_score = vector_store.similarity_search_with_relevance_scores(\n",
    "    sample_query, k=TOP_K\n",
    ")\n",
    "\n",
    "print(\"ğŸ“Š Similarity Search Results:\")\n",
    "for i, (doc, score) in enumerate(context_docs_with_score, 1):\n",
    "    print(f\"{i}. Score: {score:.3f} | Source: {doc.metadata['source']} | ChunkID: {doc.metadata['chunk_id']} \")\n",
    "    print(f\"   Preview: {doc.page_content[:50].strip()}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. MMR (Maximal Marginal Relevance) ğŸ­\n",
    "\n",
    "**ğŸ” What it is:**  Balances relevance with diversity to avoid near-duplicate results.\n",
    "\n",
    "**Formula:** `score = Î» * similarity(query, doc) â€“ (1 â€“ Î») * max(similarity(doc, selected_docs))`\n",
    "\n",
    "**ğŸ¯ When to use:**\n",
    "- When documents have repetitive/similar content\n",
    "- Need diverse perspectives on the same topic\n",
    "- Quality over quantity approach\n",
    "\n",
    "**ğŸŒŸ Pros:** Reduces redundancy, increases content diversity  \n",
    "**âš ï¸ Cons:** May miss highly relevant but similar content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ­ MMR Search Results:\n",
      "\n",
      "1. MMR Score: 0.524 | Source: Statistics_2025_Estimating_the_average_treatment_effect_in_cluster-randomized_trials___with_misc.pdf | ChunkID: 0fd71936-03f5-497e-ac0d-e63f946e558b \n",
      "   Preview: Statistical Methods in Medical Research, 22(3):278...\n",
      "\n",
      "2. MMR Score: 0.469 | Source: Generative_AI_2025_Aligning_the_Evaluation_of_Probabilistic_Predictions_with_Downstream___Value.pdf | ChunkID: 8fa0aa27-cbc9-40e7-afec-76bc574d3f5d \n",
      "   Preview: âˆ— Corresponding Author. Email: novin.shahroudi@ut....\n",
      "\n",
      "3. MMR Score: 0.412 | Source: Analytics_2025_A_Human-In-The-Loop_Approach_for_Improving_Fairness_in_Predictive___Business_Pro.pdf | ChunkID: 1fcac677-6d85-46fb-bebd-f7083b6fb815 \n",
      "   Preview: Definition 2. A trace Ïƒ = âŸ¨e1,...,enâŸ© is a finite ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MMR search\n",
    "query_embedding = embeddings.embed_query(sample_query)\n",
    "mmr_docs_with_score = vector_store.max_marginal_relevance_search_with_score_by_vector(\n",
    "    embedding=query_embedding, k=TOP_K\n",
    ")\n",
    "\n",
    "print(\"ğŸ­ MMR Search Results:\\n\")\n",
    "for i, (doc, score) in enumerate(mmr_docs_with_score, 1):\n",
    "    print(f\"{i}. MMR Score: {score:.3f} | Source: {doc.metadata['source']} | ChunkID: {doc.metadata['chunk_id']} \")\n",
    "    print(f\"   Preview: {doc.page_content[:50]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Comparison:\n",
      "Similarity sources: ['0fd71936-03f5-497e-ac0d-e63f946e558b', '8fa0aa27-cbc9-40e7-afec-76bc574d3f5d', '1661a222-0d47-4013-85d2-67864e5a9651']\n",
      "MMR sources:        ['0fd71936-03f5-497e-ac0d-e63f946e558b', '8fa0aa27-cbc9-40e7-afec-76bc574d3f5d', '1fcac677-6d85-46fb-bebd-f7083b6fb815']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ” Comparison:\")\n",
    "print(\"Similarity sources:\", [doc[0].metadata['chunk_id'] for doc in context_docs_with_score])\n",
    "print(\"MMR sources:       \", [doc[0].metadata['chunk_id'] for doc in mmr_docs_with_score])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Dynamic Top-k / Adaptive Thresholds ğŸ“\n",
    "\n",
    "**ğŸ” What it is:**  Only return results above a minimum relevance score.\n",
    "\n",
    "**ğŸ¯ When to use:**\n",
    "- Quality control - avoid irrelevant results\n",
    "- When \"no answer\" is better than a bad answer\n",
    "- Variable-length result sets based on quality\n",
    "\n",
    "**ğŸŒŸ Pros:** Ensures quality, filters noise  \n",
    "**âš ï¸ Cons:** May return no results for difficult queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Score Threshold Results (min score: 0.35):\n",
      "Found 3 documents above threshold\n",
      "All scores: [0.762209623122477, 0.7345877811591007, 0.7236573519028131]\n",
      "Above threshold (0.35): [0.762209623122477, 0.7345877811591007, 0.7236573519028131]\n"
     ]
    }
   ],
   "source": [
    "# Score thresholding with fixed k\n",
    "retriever_with_threshold = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": TOP_K, \"score_threshold\": SCORE_THRESHOLD}\n",
    ")\n",
    "threshold_docs = retriever_with_threshold.invoke(sample_query)\n",
    "\n",
    "print(f\"ğŸ“ Score Threshold Results (min score: {SCORE_THRESHOLD}):\")\n",
    "print(f\"Found {len(threshold_docs)} documents above threshold\")\n",
    "\n",
    "# Compare with scores\n",
    "all_scores = [score for _, score in context_docs_with_score]\n",
    "above_threshold = [score for score in all_scores if score >= SCORE_THRESHOLD]\n",
    "\n",
    "print(f\"All scores: {all_scores}\")\n",
    "print(f\"Above threshold ({SCORE_THRESHOLD}): {above_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results for different queries:\n",
      "Query 1: 3 results |  Scores: [0.76, 0.73, 0.72] | Topic: How does MC3G improve upon existing counterfactual...\n",
      "Query 2: 3 results |  Scores: [0.79, 0.77, 0.77] | Topic: What limitations of the Copas-Jackson bound did th...\n",
      "Query 3: 3 results |  Scores: [0.69, 0.68, 0.68] | Topic: What is the key innovation of the proposed Riemann...\n"
     ]
    }
   ],
   "source": [
    "# Score thresholding with fixed k -  With scores\n",
    "\n",
    "# Test with different queries to show variability\n",
    "print(\"\\nğŸ“Š Results for different queries:\")\n",
    "for i, query_data in enumerate(test_queries[:3], 1):\n",
    "    query = query_data[\"user_input\"]\n",
    "    docs =  vector_store.similarity_search_with_relevance_scores(query, k=TOP_K)\n",
    "    scores = [round(doc[1],2) for doc in docs]\n",
    "    print(f\"Query {i}: {len(docs)} results |  Scores: {scores} | Topic: {query[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Dynamic/Adaptive Retrieval ğŸ”„\n",
    "\n",
    "**ğŸ” What it is:**  Variable number of results based only on score threshold (no fixed k).\n",
    "\n",
    "**ğŸ¯ When to use:**\n",
    "- Query difficulty varies widely\n",
    "- Want all relevant content above a quality bar\n",
    "- Adaptive context length for LLM processing\n",
    "\n",
    "**ğŸŒŸ Pros:** Flexible result count, quality-focused  \n",
    "**âš ï¸ Cons:** Unpredictable context length, potential cost implications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Dynamic Retrieval Results:\n",
      "Found 4 documents above threshold 0.35\n",
      "\n",
      "ğŸ“Š Results for different queries:\n",
      "Query 1: 4 results | Topic: How does MC3G improve upon existing counterfactual...\n",
      "Query 2: 4 results | Topic: What limitations of the Copas-Jackson bound did th...\n",
      "Query 3: 4 results | Topic: What is the key innovation of the proposed Riemann...\n"
     ]
    }
   ],
   "source": [
    "# Dynamic retrieval (no fixed k)\n",
    "dynamic_retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"score_threshold\": SCORE_THRESHOLD}\n",
    ")\n",
    "dynamic_docs = dynamic_retriever.invoke(sample_query)\n",
    "\n",
    "print(f\"ğŸ”„ Dynamic Retrieval Results:\")\n",
    "print(f\"Found {len(dynamic_docs)} documents above threshold {SCORE_THRESHOLD}\")\n",
    "\n",
    "# Test with different queries to show variability\n",
    "print(\"\\nğŸ“Š Results for different queries:\")\n",
    "for i, query_data in enumerate(test_queries[:3], 1):\n",
    "    query = query_data[\"user_input\"]\n",
    "    docs = dynamic_retriever.invoke(query)\n",
    "    print(f\"Query {i}: {len(docs)} results | Topic: {query[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Metadata Filtering ğŸ·ï¸\n",
    "\n",
    "**ğŸ” What it is:**  Filter search results by document metadata (source, type, date, etc.).\n",
    "\n",
    "**ğŸ¯ When to use:**\n",
    "- Domain-specific searches (e.g., only medical papers)\n",
    "- Time-based filtering (recent documents only)\n",
    "- Source credibility filtering\n",
    "- User permission-based access control\n",
    "\n",
    "**ğŸŒŸ Pros:** Precise targeting, context control  \n",
    "**âš ï¸ Cons:** May miss relevant content from filtered sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Exact Source Filter Results:\n",
      "Target: Statistics_2025_Estimating_the_average_treatment_e...\n",
      "Found 3 results from this source\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Exact source match\n",
    "target_file = \"Statistics_2025_Estimating_the_average_treatment_effect_in_cluster-randomized_trials___with_misc.pdf\"\n",
    "\n",
    "exact_filter_retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": TOP_K,\n",
    "        \"filter\": Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.source\", \n",
    "                    match=MatchValue(value=target_file)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "exact_filtered_docs = exact_filter_retriever.invoke(sample_query)\n",
    "print(f\"ğŸ¯ Exact Source Filter Results:\")\n",
    "print(f\"Target: {target_file[:50]}...\")\n",
    "print(f\"Found {len(exact_filtered_docs)} results from this source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Topic Filter Results (Statistics papers):\n",
      "Found 3 results\n",
      "- Statistics_2025_Estimating_the_average_treatment_effect_in_c...\n",
      "- Statistics_2025_Estimating_the_average_treatment_effect_in_c...\n",
      "- Statistics_2025_Estimating_the_average_treatment_effect_in_c...\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Substring/topic-based filtering\n",
    "topic_filter_retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": TOP_K,\n",
    "        \"filter\": Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.source\", \n",
    "                    match=MatchText(text=\"Statistics_2025\")  # substring match\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "topic_filtered_docs = topic_filter_retriever.invoke(sample_query)\n",
    "print(f\"\\nğŸ“Š Topic Filter Results (Statistics papers):\")\n",
    "print(f\"Found {len(topic_filtered_docs)} results\")\n",
    "for doc in topic_filtered_docs:\n",
    "    print(f\"- {doc.metadata['source'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6. Document Chunk Linking ğŸ”—\n",
    "\n",
    "**ğŸ” What it is:**  First find relevant documents, then retrieve more chunks from those same documents.\n",
    "\n",
    "**ğŸ¯ When to use:**\n",
    "- When relevant info might be spread across chunks in same document\n",
    "- Want comprehensive coverage of relevant documents\n",
    "- Building document-level understanding\n",
    "\n",
    "**ğŸŒŸ Pros:** Comprehensive document coverage, maintains context  \n",
    "**âš ï¸ Cons:** May include less relevant chunks from relevant documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— Document Chunk Linking:\n",
      "Step 1 - Found relevant documents:\n",
      "- Statistics_2025_Estimating_the_average_treatment_effect_in_c...\n",
      "- Generative_AI_2025_Aligning_the_Evaluation_of_Probabilistic_...\n",
      "\n",
      "Step 2 - Retrieved 3 total chunks from relevant documents\n",
      "- 1 chunks from Statistics_2025_Estimating_the_average_treatment_e...\n",
      "- 2 chunks from Generative_AI_2025_Aligning_the_Evaluation_of_Prob...\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Find most relevant documents\n",
    "initial_docs = vector_store.similarity_search(sample_query, k=2)\n",
    "relevant_sources = [doc.metadata[\"source\"] for doc in initial_docs]\n",
    "\n",
    "print(f\"ğŸ”— Document Chunk Linking:\")\n",
    "print(f\"Step 1 - Found relevant documents:\")\n",
    "for source in relevant_sources:\n",
    "    print(f\"- {source[:60]}...\")\n",
    "\n",
    "# Step 2: Get more chunks from these documents\n",
    "linked_retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": TOP_K,\n",
    "        \"filter\": Filter(\n",
    "            should=[  # OR condition across multiple files\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.source\",\n",
    "                    match=MatchValue(value=source)\n",
    "                )\n",
    "                for source in relevant_sources\n",
    "            ]\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "linked_docs = linked_retriever.invoke(sample_query)\n",
    "print(f\"\\nStep 2 - Retrieved {len(linked_docs)} total chunks from relevant documents\")\n",
    "\n",
    "# Show distribution\n",
    "from collections import Counter\n",
    "source_counts = Counter([doc.metadata['source'] for doc in linked_docs])\n",
    "for source, count in source_counts.items():\n",
    "    print(f\"- {count} chunks from {source[:50]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LLM-Guided Filtering ğŸ¤–\n",
    "\n",
    "**ğŸ” What it is:**  Use an LLM to classify queries and route to appropriate filtered retrievers.\n",
    "\n",
    "**ğŸ¯ When to use:**\n",
    "- Multi-domain knowledge bases\n",
    "- Complex query understanding needed\n",
    "- When simple keyword filtering isn't sufficient\n",
    "- Domain-specific optimization\n",
    "\n",
    "**ğŸŒŸ Pros:** Intelligent routing, domain optimization  \n",
    "**âš ï¸ Cons:** Added LLM call overhead, potential classification errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing LLM-Guided Filtering:\n",
      "\n",
      "\n",
      "ğŸ“‹ Test 1: How does MC3G improve upon existing counterfactual explanation methods, particularly concerning cost computation and causal dependencies?\n",
      "ğŸ¤– Predicted: Other | Expected: Other\n",
      "Correctly Classified âœ…\n",
      "\n",
      "ğŸ“‹ Test 2: What limitations of the Copas-Jackson bound did the authors identify, and how did they address these limitations in their proposed method?\n",
      "ğŸ¤– Predicted: Other | Expected: Other\n",
      "Correctly Classified âœ…\n",
      "\n",
      "ğŸ“‹ Test 3: What is the key innovation of the proposed Riemannian change point detection method, and how does it improve upon previous approaches?\n",
      "ğŸ¤– Predicted: Other | Expected: Other\n",
      "Correctly Classified âœ…\n",
      "\n",
      "ğŸ“‹ Test 4: What are the key challenges in adapting existing open-vocabulary semantic segmentation (OVSS) frameworks, designed for natural images, to remote sensi\n",
      "ğŸ¤– Predicted: Computer_Vision | Expected: Computer_Vision\n",
      "Correctly Classified âœ…\n",
      "\n",
      "ğŸ“‹ Test 5: What are the key decidable restrictions on the SSM satisfiability problem identified in the paper, and what are their corresponding complexity bounds?\n",
      "ğŸ¤– Predicted: Other | Expected: Other\n",
      "Correctly Classified âœ…\n"
     ]
    }
   ],
   "source": [
    "# Setup LLM for query classification\n",
    "llm = model_manager.get_llm()\n",
    "\n",
    "TOPIC_IDENTIFICATION_TEMPLATE = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"\"\"\n",
    "Classify the query into one of two topics:\n",
    "\n",
    "- \"Computer_Vision\": if related to image/video processing, recognition, detection, segmentation, or OCR.  \n",
    "- \"Other\": for everything else.  \n",
    "\n",
    "Return only the class name.  \n",
    "\n",
    "Query: {query}  \n",
    "Topic:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def classify_query(query: str) -> str:\n",
    "    \"\"\"Classify query into Computer_Vision or Other.\"\"\"\n",
    "    category = llm.invoke(\n",
    "        TOPIC_IDENTIFICATION_TEMPLATE.format(query=query)\n",
    "    ).content.strip()\n",
    "    return category\n",
    "\n",
    "def get_retriever(category: str, k: int = TOP_K):\n",
    "    \"\"\"Return retriever based on classification.\"\"\"\n",
    "    if category == \"Computer_Vision\":\n",
    "        print(\"ğŸ¯ Using Computer Vision filtered retriever\\n\")\n",
    "        return vector_store.as_retriever(\n",
    "            search_kwargs={\n",
    "                \"k\": k,\n",
    "                \"filter\": Filter(\n",
    "                    must=[\n",
    "                        FieldCondition(\n",
    "                            key=\"metadata.source\", \n",
    "                            match=MatchText(text=\"Computer_Vision\")\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        print(\"ğŸ” Using general retriever\\n\")\n",
    "        return vector_store.as_retriever(\n",
    "            search_kwargs={\"k\": k, \"score_threshold\": SCORE_THRESHOLD}\n",
    "        )\n",
    "\n",
    "# -------------------\n",
    "# Testing\n",
    "# -------------------\n",
    "print(\"ğŸ§ª Testing LLM-Guided Filtering:\\n\")\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    q_text = query[\"user_input\"]\n",
    "    expected = \"Computer_Vision\" if \"Computer_Vision\" in query[\"source_file\"] else \"Other\"\n",
    "\n",
    "    print(f\"\\nğŸ“‹ Test {i}: {q_text[:150]}\")\n",
    "\n",
    "    predicted = classify_query(q_text)\n",
    "    print(f\"ğŸ¤– Predicted: {predicted} | Expected: {expected}\")\n",
    "\n",
    "    result = \"Correctly Classified âœ…\" if predicted == expected else \"Incorrectly Classified âŒ\"\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reranking ğŸ”\n",
    "\n",
    "**ğŸ” What it is:**  \n",
    "Applies a secondary model (e.g., cross-encoder, LLM, or relevance model) to reorder initial retrieval results, improving relevance ranking beyond similarity scores alone.  \n",
    "\n",
    "**ğŸ¯ When to use:**   \n",
    "- Dense or hybrid retrieval gives many candidates but order matters  \n",
    "- Need **higher precision** in the top results (e.g., top-3 for RAG context)  \n",
    "- Queries where subtle semantic nuances are important  \n",
    "\n",
    "**ğŸŒŸ Pros:**   \n",
    "- Improves relevance of top results  \n",
    "- Reduces noise passed to the LLM  \n",
    "- Works well with hybrid/similarity search as a post-processing step  \n",
    "\n",
    "**âš ï¸ Cons:**   \n",
    "- Higher latency and compute cost  \n",
    "- Requires additional model training or fine-tuning for best results  \n",
    "- May not scale well for very large candidate sets  \n",
    "\n",
    "**Example use case:**  \n",
    "- Initial retriever returns 20 (>>K) research paper abstracts.  \n",
    "- A cross-encoder reranker re-scores them, surfacing the **most directly relevant Top-K** for the query context.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer reranking.py \n",
    "def _setup_reranker_retriever():\n",
    "    \"\"\"Initialize the reranking retriever.\"\"\"\n",
    "    qdrant_path = config.QDRANT_DIR / strategy.value\n",
    "    qdrant_client = QdrantClient(path=str(qdrant_path))\n",
    "    embeddings = model_manager.get_embeddings()\n",
    "\n",
    "    vector_store = QdrantVectorStore(\n",
    "        client=qdrant_client,\n",
    "        collection_name=strategy.value,\n",
    "        embedding=embeddings,\n",
    "    )\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": top_k})\n",
    "\n",
    "    model = HuggingFaceCrossEncoder(model_name=constants.RERANKER_MODEL)\n",
    "    compressor = CrossEncoderReranker(model=model, top_n=top_n)\n",
    "    reranker_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor, base_retriever=retriever\n",
    "    )\n",
    "    return reranker_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Reranker initialized\n",
      "ğŸ” Starting reranking evaluation...\n",
      "ğŸ“Š Evaluating 5 test queries\n",
      "\n",
      "ğŸ“ Query 1: How does MC3G improve upon existing counterfactual explanation methods, particularly concerning cost...\n",
      "  ğŸ“‹ Result 1: Reranker=0.418, Baseline=0.418\n",
      "  ğŸ“‹ Result 2: Reranker=0.384, Baseline=0.384\n",
      "  ğŸ“‹ Result 3: Reranker=0.376, Baseline=0.419\n",
      "  ğŸ“Š Query avg: Reranker=0.393, Baseline=0.407\n",
      "\n",
      "ğŸ“ Query 2: What limitations of the Copas-Jackson bound did the authors identify, and how did they address these...\n",
      "  ğŸ“‹ Result 1: Reranker=0.852, Baseline=0.863\n",
      "  ğŸ“‹ Result 2: Reranker=0.843, Baseline=0.803\n",
      "  ğŸ“‹ Result 3: Reranker=0.863, Baseline=0.852\n",
      "  ğŸ“Š Query avg: Reranker=0.853, Baseline=0.839\n",
      "\n",
      "ğŸ“ Query 3: What is the key innovation of the proposed Riemannian change point detection method, and how does it...\n",
      "  ğŸ“‹ Result 1: Reranker=0.266, Baseline=0.397\n",
      "  ğŸ“‹ Result 2: Reranker=0.263, Baseline=0.266\n",
      "  ğŸ“‹ Result 3: Reranker=0.272, Baseline=0.241\n",
      "  ğŸ“Š Query avg: Reranker=0.267, Baseline=0.301\n",
      "\n",
      "ğŸ“ Query 4: What are the key challenges in adapting existing open-vocabulary semantic segmentation (OVSS) framew...\n",
      "  ğŸ“‹ Result 1: Reranker=0.369, Baseline=0.405\n",
      "  ğŸ“‹ Result 2: Reranker=0.256, Baseline=0.369\n",
      "  ğŸ“‹ Result 3: Reranker=0.341, Baseline=0.315\n",
      "  ğŸ“Š Query avg: Reranker=0.322, Baseline=0.363\n",
      "\n",
      "ğŸ“ Query 5: What are the key decidable restrictions on the SSM satisfiability problem identified in the paper, a...\n",
      "  ğŸ“‹ Result 1: Reranker=0.290, Baseline=0.355\n",
      "  ğŸ“‹ Result 2: Reranker=0.320, Baseline=0.290\n",
      "  ğŸ“‹ Result 3: Reranker=0.310, Baseline=0.354\n",
      "  ğŸ“Š Query avg: Reranker=0.307, Baseline=0.333\n",
      "\n",
      "==================================================\n",
      "ğŸ¯ FINAL RESULTS\n",
      "==================================================\n",
      "ğŸ“ˆ Reranker Average Score:  0.4281\n",
      "ğŸ“Š Baseline Average Score:  0.4488\n",
      "ğŸš€ Improvement:             -0.0206\n",
      "ğŸ“Š Improvement Percentage:  -4.59%\n",
      "âŒ Reranking shows no improvement\n"
     ]
    }
   ],
   "source": [
    "from retrieval_playground.src.mid_retrieval.reranking import Reranker\n",
    "reranker = Reranker(qdrant_client=qdrant_client, top_n=TOP_K)\n",
    "reranker_evaluation_results = reranker.evaluate_reranking()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hybrid Filtering ğŸ”—\n",
    "\n",
    "**ğŸ” What it is:**   \n",
    "Combines sparse retrieval (BM25/keyword-based) with dense retrieval (embeddings-based) to leverage both exact keyword matching and semantic similarity.\n",
    "\n",
    "**How it works:**  \n",
    "- BM25 ensures keyword precision (great for rare terms, acronyms, or exact matches).  \n",
    "- Dense retrieval ensures semantic recall (captures meaning even if words differ).  \n",
    "- Final results are combined (via weighted scores, reranking, or union).  \n",
    "\n",
    "**ğŸ¯ When to use:**  \n",
    "- Queries with a mix of rare keywords and semantic intent  \n",
    "- Domain-specific content with technical jargon (BM25 helps catch exact terms)  \n",
    "- General RAG pipelines where coverage + precision both matter  \n",
    "- When neither sparse nor dense alone gives consistently good results  \n",
    "\n",
    "**ğŸŒŸ Pros:** Best of both worlds â€“ exact match + semantic understanding  \n",
    "**âš ï¸ Cons:** More complex to implement, higher compute cost  \n",
    "\n",
    "**Example use case:**  \n",
    "\n",
    "*Query:*  \n",
    "*â€œWhat are the key challenges in adapting existing open-vocabulary semantic segmentation (OVSS) frameworks, designed for natural images, to remote sensing images, and how does SegEarth-OV address these challenges?â€*  \n",
    "\n",
    "*BM25 (Sparse) ğŸ§©*: Catches exact terms - *â€œsemantic segmentationâ€*, *â€œopen-vocabularyâ€*, *â€œremote sensingâ€*, *OVSS*  \n",
    "\n",
    "*Dense (Semantic) ğŸ§ *: Finds paraphrases - *â€œpixel-level classification for satellite imageryâ€*, *â€œdomain adaptation from natural to aerial scenesâ€*, *â€œgeneralized segmentation across modalitiesâ€*  \n",
    "\n",
    "*Hybrid âš¡*: Returns both - keyword-heavy matches (*â€œopen-vocabulary segmentation for remote sensingâ€*) + semantic ones (*â€œSegEarth-OV enables cross-domain satellite segmentationâ€*).  \n",
    "\n",
    "*Result:* Covers **precision (keywords)** + **recall (semantic similarity)**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.models import PointStruct, SparseVector, NamedVector, NamedSparseVector, SearchRequest\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from splade.models.transformer import Splade\n",
    "import torch\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Setup Qdrant client\n",
    "# ----------------------------\n",
    "client = QdrantClient(url={url})\n",
    "\n",
    "collection = {add_collection_name}\n",
    "\n",
    "# Create collection with dense + sparse configs\n",
    "client.recreate_collection(\n",
    "    collection_name=collection,\n",
    "    vectors_config={\"text_dense\": models.VectorParams(size=768, distance=models.Distance.COSINE)},\n",
    "    sparse_vectors_config={\"text_sparse\": models.SparseVectorParams()}\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Load embedding models\n",
    "# ----------------------------\n",
    "dense_model = SentenceTransformer({DENSE_EMBEDDING})   # Dense embeddings\n",
    "sparse_model = Splade(\"naver/splade-cocondenser-ensembledistil\")  # Sparse embeddings\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Collection Update: Insert example documents\n",
    "# ----------------------------\n",
    "docs = [\n",
    "    \"Estimating treatment effect in cluster randomized trials\",\n",
    "    \"Neural networks for computer vision tasks\",\n",
    "    \"Advancements in generative AI and LLMs\"\n",
    "]\n",
    "\n",
    "points = []\n",
    "for i, text in enumerate(docs):\n",
    "    dense_vec = dense_model.encode(text).tolist()\n",
    "    with torch.no_grad():\n",
    "        sparse_out = sparse_model(text)\n",
    "    indices = sparse_out.indices.tolist()\n",
    "    values = sparse_out.values.tolist()\n",
    "\n",
    "    points.append(\n",
    "        PointStruct(\n",
    "            id=i,\n",
    "            payload={\"text\": text},\n",
    "            vector={\n",
    "                \"text_dense\": dense_vec,\n",
    "                \"text_sparse\": SparseVector(indices=indices, values=values),\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "client.upsert(collection_name=collection, points=points)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Query Input\n",
    "# ----------------------------\n",
    "query = input(\"Enter your query: \")\n",
    "\n",
    "# Dense vector\n",
    "query_dense = dense_model.encode(query).tolist()\n",
    "\n",
    "# Sparse vector\n",
    "with torch.no_grad():\n",
    "    sparse_out = sparse_model(query)\n",
    "query_sparse = SparseVector(\n",
    "    indices=sparse_out.indices.tolist(),\n",
    "    values=sparse_out.values.tolist()\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Hybrid Search (Batch)\n",
    "# ----------------------------\n",
    "response = client.search_batch(\n",
    "    collection_name=collection,\n",
    "    requests=[\n",
    "        SearchRequest(vector=NamedVector(name=\"text_dense\", vector=query_dense), limit=5),\n",
    "        SearchRequest(vector=NamedSparseVector(name=\"text_sparse\", vector=query_sparse), limit=5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dense_results, sparse_results = response\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Fuse Results (Reciprocal Rank Fusion)\n",
    "# ----------------------------\n",
    "combined_scores = {}\n",
    "\n",
    "for rank, res in enumerate(dense_results):\n",
    "    combined_scores[res.id] = combined_scores.get(res.id, 0) + 1 / (60 + rank)\n",
    "for rank, res in enumerate(sparse_results):\n",
    "    combined_scores[res.id] = combined_scores.get(res.id, 0) + 1 / (60 + rank)\n",
    "\n",
    "fused_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Print Results\n",
    "# ----------------------------\n",
    "print(\"\\nğŸ” Hybrid Search Results:\")\n",
    "for doc_id, score in fused_results:\n",
    "    print(f\"Score: {score:.4f} | Text: {docs[doc_id]}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Method Comparison & Recommendations\n",
    "\n",
    "### ğŸ“Š Quick Comparison Table\n",
    "\n",
    "| Method | Best For | Pros | Cons | Complexity |\n",
    "|--------|----------|------|------|------------|\n",
    "| **Similarity Search** | General purpose | Fast, simple, reliable | May return duplicates | Low |\n",
    "| **MMR** | Diverse content | Reduces redundancy | May miss similar relevant content | Medium |\n",
    "| **Score Threshold** | Quality control | Ensures minimum quality | May return no results | Low |\n",
    "| **Dynamic Retrieval** | Variable content needs | Flexible, quality-focused | Unpredictable context length | Low |\n",
    "| **Metadata Filtering** | Domain-specific | Precise targeting | May miss cross-domain insights | Medium |\n",
    "| **Chunk Linking** | Document comprehension | Comprehensive coverage | Less relevant chunks included | Medium |\n",
    "| **LLM-Guided** | Multi-domain systems | Intelligent routing | LLM overhead, classification errors | High |\n",
    "| **Hybrid Retrieval (BM25 + Dense)** | Balanced semantic + keyword search | Captures both exact matches & semantic meaning | Requires tuning weight between BM25 & dense | High |\n",
    "| **Reranking** | Precision in top results | Improves top-k relevance, reduces noise | Higher latency & compute cost | High |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Recommendations\n",
    "\n",
    "#### ğŸš€ **Start Simple**\n",
    "Begin with **Similarity Search + Score Threshold** for most applications.\n",
    "\n",
    "#### ğŸ“ˆ **Scale Up Based on Needs**\n",
    "- Add **MMR** if you notice repetitive results  \n",
    "- Use **Metadata Filtering** for multi-domain knowledge bases  \n",
    "- Implement **LLM-Guided Filtering** for complex routing needs  \n",
    "- Adopt **Hybrid Retrieval (BM25 + Dense)** when queries require both exact keyword matches (e.g., specific terms, codes) and semantic understanding (contextual intent)  \n",
    "- Add **Reranking** when you need **high precision in the top results** (e.g., top-3 context for LLMs)  \n",
    "\n",
    "#### ğŸ”§ **Fine-tuning Tips**\n",
    "- Adjust `score_threshold` based on your quality requirements (0.3â€“0.5 is typical)  \n",
    "- Use **Dynamic Retrieval** when context length flexibility is valuable  \n",
    "- Apply **Chunk Linking** for document-centric tasks  \n",
    "- Tune the BM25 vs Dense weighting in **Hybrid Retrieval** (common ranges: 0.3â€“0.7) depending on whether precision (keywords) or recall (semantics) is more important  \n",
    "- Use **Reranking** selectively on top-N candidates to balance cost vs accuracy  \n",
    "\n",
    "#### ğŸ’¡ **Pro Tips**\n",
    "- Monitor retrieval metrics (precision, recall) to choose optimal methods  \n",
    "- Consider combining methods (e.g., MMR + Score Threshold, Hybrid + Reranking)  \n",
    "- Cache LLM classification results for repeated query patterns  \n",
    "- A/B test different approaches with your specific use case  \n",
    "- Start with simple methods and add complexity only when needed  \n",
    "\n",
    "#### ğŸ¯ **Common Patterns**\n",
    "- **High-quality RAG**: Similarity Search + Score Threshold + MMR  \n",
    "- **Multi-domain KB**: LLM-Guided Filtering + Metadata Filtering  \n",
    "- **Document Analysis**: Chunk Linking + Score Threshold  \n",
    "- **Exploratory Search**: Dynamic Retrieval + MMR  \n",
    "- **Balanced Search (keyword + meaning)**: Hybrid Retrieval (BM25 + Dense) + Score Threshold  \n",
    "- **High-precision Context**: Hybrid Retrieval + Reranking  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âœ… Tutorial completed! You now know 9 different retrieval methods.\n",
    "ğŸš€ Ready to build better RAG systems!\n",
    "\n",
    "ğŸ¯ Next steps:\n",
    "- Experiment with different methods on your own data\n",
    "- Combine methods for optimal results\n",
    "- Monitor and evaluate retrieval quality\n",
    "- Scale complexity based on your specific needs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
