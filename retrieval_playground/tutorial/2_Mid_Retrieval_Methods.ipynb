{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Methods Tutorial\n",
    "\n",
    "This notebook demonstrates various retrieval methods for RAG systems using vector databases. Each method serves different use cases and has specific advantages.\n",
    "\n",
    "## Overview of Methods\n",
    "\n",
    "1. **Basic Similarity Search** - Standard semantic search\n",
    "2. **MMR (Maximal Marginal Relevance)** - Balance relevance and diversity\n",
    "3. **Score Thresholding** - Quality-based filtering\n",
    "4. **Adaptive/Dynamic Retrieval** - Flexible result counts\n",
    "5. **Metadata Filtering** - Context-aware search\n",
    "6. **Document Chunk Linking** - Multi-document retrieval\n",
    "7. **LLM-Guided Filtering** - Intelligent pre-filtering\n",
    "8. **Reranking** - Reorder results for higher precision  \n",
    "9. **Hybrid Retrieval** - Combine keyword and semantic search  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, Filter, FieldCondition, MatchValue, MatchText\n",
    "from retrieval_playground.utils import config, constants\n",
    "from retrieval_playground.utils.model_manager import model_manager\n",
    "from retrieval_playground.src.pre_retrieval.chunking_strategies import ChunkingStrategy\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import Dict, List, Any\n",
    "import json\n",
    "import gc\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample query: How does MC3G improve upon existing counterfactual explanation methods, particularly concerning cost computation and causal dependencies?\n"
     ]
    }
   ],
   "source": [
    "# Load test queries\n",
    "def load_test_queries() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load test queries from JSON file.\"\"\"\n",
    "    queries_path = config.TESTS_DIR / \"test_queries.json\"\n",
    "    with open(queries_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "test_queries = load_test_queries()\n",
    "sample_query = test_queries[0][\"user_input\"]\n",
    "print(f\"Sample query: {sample_query}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-22 15:17:23.059 INFO model_manager - _initialize_models: üîÑ ModelManager: Initializing shared AI models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 15:17:23,059 - INFO - üîÑ ModelManager: Initializing shared AI models...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-22 15:17:26.967 INFO model_manager - _initialize_models: ‚úÖ ModelManager: Shared AI models initialized successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 15:17:26,967 - INFO - ‚úÖ ModelManager: Shared AI models initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized!\n"
     ]
    }
   ],
   "source": [
    "# Setup vector database\n",
    "strategy = ChunkingStrategy.UNSTRUCTURED\n",
    "qdrant_client = QdrantClient(url=constants.QDRANT_URL, api_key=constants.QDRANT_KEY)\n",
    "embeddings = model_manager.get_embeddings()\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=strategy.value,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(\"Vector store initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters\n",
    "TOP_K = 3\n",
    "SCORE_THRESHOLD = 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Similarity Search\n",
    "\n",
    "**What it is:**  Standard cosine similarity between query and document embeddings.\n",
    "\n",
    "**When to use:**\n",
    "- Default choice for most RAG applications\n",
    "- When you need the most semantically similar content\n",
    "- Simple, fast, and reliable\n",
    "\n",
    "**‚úÖ Pros:** Fast, simple, works well for most cases  \n",
    "**‚ö†Ô∏è Cons:** May return very similar/duplicate content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Similarity Search Results:\n",
      "1. Score: 0.916 | Source: Analytics_2025_MC3G__Model_Agnostic_Causally_Constrained_Counterfactual_Generation.pdf | ChunkID: 64ab7c14-a35a-4d58-846c-c0562c91c111 \n",
      "   Preview: Cars\n",
      "\n",
      "6. Conclusion and Future Work In this paper,...\n",
      "\n",
      "2. Score: 0.910 | Source: Analytics_2025_MC3G__Model_Agnostic_Causally_Constrained_Counterfactual_Generation.pdf | ChunkID: f8328dda-95ac-4243-afd8-1e233f5d8e4b \n",
      "   Preview: 3.2. MC3G Approach MC3G defines two distinct state...\n",
      "\n",
      "3. Score: 0.903 | Source: Analytics_2025_MC3G__Model_Agnostic_Causally_Constrained_Counterfactual_Generation.pdf | ChunkID: 0ed01265-aeea-4eb1-8a74-951aeb822b89 \n",
      "   Preview: Causally Compliant Causal Consistency (%)\n",
      "\n",
      "FALSE I...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic similarity search with scores\n",
    "context_docs_with_score = vector_store.similarity_search_with_relevance_scores(\n",
    "    sample_query, k=TOP_K\n",
    ")\n",
    "\n",
    "print(\"üìä Similarity Search Results:\")\n",
    "for i, (doc, score) in enumerate(context_docs_with_score, 1):\n",
    "    print(f\"{i}. Score: {score:.3f} | Source: {doc.metadata['source']} | ChunkID: {doc.metadata['chunk_id']} \")\n",
    "    print(f\"   Preview: {doc.page_content[:50].strip()}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MMR (Maximal Marginal Relevance)\n",
    "\n",
    "**What it is:**  Balances relevance with diversity to avoid near-duplicate results.\n",
    "\n",
    "**Formula:** `score = Œª * similarity(query, doc) ‚Äì (1 ‚Äì Œª) * max(similarity(doc, selected_docs))`\n",
    "\n",
    "**When to use:**\n",
    "- When documents have repetitive/similar content\n",
    "- Need diverse perspectives on the same topic\n",
    "- Quality over quantity approach\n",
    "\n",
    "**‚úÖ Pros:** Reduces redundancy, increases content diversity  \n",
    "**‚ö†Ô∏è Cons:** May miss highly relevant but similar content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMR Search Results:\n",
      "\n",
      "1. MMR Score: 0.833 | Source: Analytics_2025_MC3G__Model_Agnostic_Causally_Constrained_Counterfactual_Generation.pdf | ChunkID: 64ab7c14-a35a-4d58-846c-c0562c91c111 \n",
      "   Preview: Cars\n",
      "\n",
      "6. Conclusion and Future Work In this paper,...\n",
      "\n",
      "2. MMR Score: 0.429 | Source: Analytics_2025_Graph-R1__Incentivizing_the_Zero-Shot_Graph_Learning_Capability_in_LLMs___via_Ex.pdf | ChunkID: a28cf9c3-9ab2-493e-9a6a-65b32e1e4638 \n",
      "   Preview: Content\n",
      "\n",
      "Calculate the chemical relevant propertie...\n",
      "\n",
      "3. MMR Score: 0.403 | Source: Statistics_2025_Copas-Jackson-type_bounds_for_publication_bias_over_a_general_class_of___selecti.pdf | ChunkID: 6c302b0b-ce7c-46cc-b37e-095543928f2a \n",
      "   Preview: Marginal selection probability\n",
      "\n",
      "Figure 1. Variatio...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MMR search\n",
    "query_embedding = embeddings.embed_query(sample_query)\n",
    "mmr_docs_with_score = vector_store.max_marginal_relevance_search_with_score_by_vector(\n",
    "    embedding=query_embedding, k=TOP_K\n",
    ")\n",
    "\n",
    "print(\"MMR Search Results:\\n\")\n",
    "for i, (doc, score) in enumerate(mmr_docs_with_score, 1):\n",
    "    print(f\"{i}. MMR Score: {score:.3f} | Source: {doc.metadata['source']} | ChunkID: {doc.metadata['chunk_id']} \")\n",
    "    print(f\"   Preview: {doc.page_content[:50]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison:\n",
      "Similarity sources: ['64ab7c14-a35a-4d58-846c-c0562c91c111', 'f8328dda-95ac-4243-afd8-1e233f5d8e4b', '0ed01265-aeea-4eb1-8a74-951aeb822b89']\n",
      "MMR sources:        ['64ab7c14-a35a-4d58-846c-c0562c91c111', 'a28cf9c3-9ab2-493e-9a6a-65b32e1e4638', '6c302b0b-ce7c-46cc-b37e-095543928f2a']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComparison:\")\n",
    "print(\"Similarity sources:\", [doc[0].metadata['chunk_id'] for doc in context_docs_with_score])\n",
    "print(\"MMR sources:       \", [doc[0].metadata['chunk_id'] for doc in mmr_docs_with_score])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del context_docs_with_score, query_embedding, mmr_docs_with_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dynamic / Adaptive Thresholds\n",
    "\n",
    "**What it is:**  Only return results above a minimum relevance score.\n",
    "\n",
    "**When to use:**\n",
    "- Quality control - avoid irrelevant results\n",
    "- When \"no answer\" is better than a bad answer\n",
    "- Variable-length result sets based on quality\n",
    "\n",
    "**‚úÖ Pros:** Ensures quality, filters noise  \n",
    "**‚ö†Ô∏è Cons:** May return no results for difficult queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè Score Threshold Results (min score: 0.5):\n",
      "Found 3 documents above threshold\n",
      "All scores: [0.9163227700000001, 0.91010773, 0.90327715]\n",
      "Above threshold (0.5): [0.9163227700000001, 0.91010773, 0.90327715]\n"
     ]
    }
   ],
   "source": [
    "# Score thresholding with fixed k\n",
    "context_docs_with_score = vector_store.similarity_search_with_relevance_scores(\n",
    "    sample_query, k=TOP_K\n",
    ")\n",
    "\n",
    "retriever_with_threshold = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": TOP_K, \"score_threshold\": SCORE_THRESHOLD}\n",
    ")\n",
    "threshold_docs = retriever_with_threshold.invoke(sample_query)\n",
    "\n",
    "print(f\"üìè Score Threshold Results (min score: {SCORE_THRESHOLD}):\")\n",
    "print(f\"Found {len(threshold_docs)} documents above threshold\")\n",
    "\n",
    "# Compare with scores\n",
    "all_scores = [score for _, score in context_docs_with_score]\n",
    "above_threshold = [score for score in all_scores if score >= SCORE_THRESHOLD]\n",
    "\n",
    "print(f\"All scores: {all_scores}\")\n",
    "print(f\"Above threshold ({SCORE_THRESHOLD}): {above_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Results for different queries:\n",
      "Query 1: 3 results |  Scores: [0.92, 0.91, 0.9] | Topic: How does MC3G improve upon existing counterfactual...\n",
      "Query 2: 3 results |  Scores: [0.79, 0.77, 0.77] | Topic: What limitations of the Copas-Jackson bound did th...\n",
      "Query 3: 3 results |  Scores: [0.89, 0.87, 0.86] | Topic: What is the key innovation of the proposed Riemann...\n"
     ]
    }
   ],
   "source": [
    "# Score thresholding with fixed k -  With scores\n",
    "# Test with different queries to show variability\n",
    "print(\"\\nüìä Results for different queries:\")\n",
    "for i, query_data in enumerate(test_queries[:3], 1):\n",
    "    query = query_data[\"user_input\"]\n",
    "    docs =  vector_store.similarity_search_with_relevance_scores(query, k=TOP_K)\n",
    "    scores = [round(doc[1],2) for doc in docs]\n",
    "    print(f\"Query {i}: {len(docs)} results |  Scores: {scores} | Topic: {query[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del context_docs_with_score, retriever_with_threshold, threshold_docs, all_scores, above_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dynamic/Adaptive Retrieval\n",
    "\n",
    "**What it is:**  Variable number of results based only on score threshold (no fixed k).\n",
    "\n",
    "**When to use:**\n",
    "- Query difficulty varies widely\n",
    "- Want all relevant content above a quality bar\n",
    "- Adaptive context length for LLM processing\n",
    "\n",
    "**‚úÖ Pros:** Flexible result count, quality-focused  \n",
    "**‚ö†Ô∏è Cons:** Unpredictable context length, potential cost implications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Dynamic Retrieval Results:\n",
      "Found 4 documents above threshold 0.5\n",
      "\n",
      "üìä Results for different queries:\n",
      "Query 1: 4 results | Topic: How does MC3G improve upon existing counterfactual...\n",
      "Query 2: 4 results | Topic: What limitations of the Copas-Jackson bound did th...\n",
      "Query 3: 4 results | Topic: What is the key innovation of the proposed Riemann...\n"
     ]
    }
   ],
   "source": [
    "# Dynamic retrieval (no fixed k)\n",
    "dynamic_retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"score_threshold\": SCORE_THRESHOLD}\n",
    ")\n",
    "dynamic_docs = dynamic_retriever.invoke(sample_query)\n",
    "\n",
    "print(f\"üîÑ Dynamic Retrieval Results:\")\n",
    "print(f\"Found {len(dynamic_docs)} documents above threshold {SCORE_THRESHOLD}\")\n",
    "\n",
    "# Test with different queries to show variability\n",
    "print(\"\\nüìä Results for different queries:\")\n",
    "for i, query_data in enumerate(test_queries[:3], 1):\n",
    "    query = query_data[\"user_input\"]\n",
    "    docs = dynamic_retriever.invoke(query)\n",
    "    print(f\"Query {i}: {len(docs)} results | Topic: {query[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dynamic_retriever, dynamic_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metadata Filtering\n",
    "\n",
    "**What it is:**  Filter search results by document metadata (source, type, date, etc.).\n",
    "\n",
    "**When to use:**\n",
    "- Domain-specific searches (e.g., only medical papers)\n",
    "- Time-based filtering (recent documents only)\n",
    "- Source credibility filtering\n",
    "- User permission-based access control\n",
    "\n",
    "**‚úÖ Pros:** Precise targeting, context control  \n",
    "**‚ö†Ô∏è Cons:** May miss relevant content from filtered sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=55, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client.http import models as rest\n",
    "\n",
    "qdrant_client.create_payload_index(\n",
    "    collection_name=\"unstructured\",\n",
    "    field_name=\"metadata.source\",\n",
    "    field_schema=rest.PayloadSchemaType.TEXT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Exact Source Filter Results:\n",
      "Target: Statistics_2025_Copas-Jackson-type_bounds_for_publ...\n",
      "Found 3 results from this source\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Exact source match\n",
    "target_file = \"Statistics_2025_Copas-Jackson-type_bounds_for_publication_bias_over_a_general_class_of___selecti.pdf\"\n",
    "\n",
    "exact_filter_retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": TOP_K,\n",
    "        \"filter\": Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.source\",  \n",
    "                    match=MatchText(text=target_file)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "exact_filtered_docs = exact_filter_retriever.invoke(sample_query)\n",
    "print(f\"üéØ Exact Source Filter Results:\")\n",
    "print(f\"Target: {target_file[:50]}...\")\n",
    "print(f\"Found {len(exact_filtered_docs)} results from this source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Topic Filter Results (Statistics papers):\n",
      "Found 3 results\n",
      "- Statistics_2025_Copas-Jackson-type_bounds_for_publication_bi...\n",
      "- Statistics_2025_Alternative_statistical_inference_for_the_fi...\n",
      "- Statistics_2025_Alternative_statistical_inference_for_the_fi...\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Substring/topic-based filtering\n",
    "topic_filter_retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": TOP_K,\n",
    "        \"filter\": Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.source\", \n",
    "                    match=MatchText(text=\"Statistics_2025\")  # substring match\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "topic_filtered_docs = topic_filter_retriever.invoke(sample_query)\n",
    "print(f\"\\nüìä Topic Filter Results (Statistics papers):\")\n",
    "print(f\"Found {len(topic_filtered_docs)} results\")\n",
    "for doc in topic_filtered_docs:\n",
    "    print(f\"- {doc.metadata['source'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del target_file, exact_filter_retriever, exact_filtered_docs, topic_filter_retriever, topic_filtered_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Document Chunk Linking\n",
    "\n",
    "**What it is:**  First find relevant documents, then retrieve more chunks from those same documents.\n",
    "\n",
    "**When to use:**\n",
    "- When relevant info might be spread across chunks in same document\n",
    "- Want comprehensive coverage of relevant documents\n",
    "- Building document-level understanding\n",
    "\n",
    "**‚úÖ Pros:** Comprehensive document coverage, maintains context  \n",
    "**‚ö†Ô∏è Cons:** May include less relevant chunks from relevant documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Document Chunk Linking:\n",
      "Step 1 - Found relevant documents:\n",
      "- Analytics_2025_MC3G__Model_Agnostic_Causally_Constrained_Cou...\n",
      "- Analytics_2025_MC3G__Model_Agnostic_Causally_Constrained_Cou...\n",
      "\n",
      "Step 2 - Retrieved 3 total chunks from relevant documents\n",
      "- 3 chunks from Analytics_2025_MC3G__Model_Agnostic_Causally_Const...\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Find most relevant documents\n",
    "initial_docs = vector_store.similarity_search(sample_query, k=2)\n",
    "relevant_sources = [doc.metadata[\"source\"] for doc in initial_docs]\n",
    "\n",
    "print(f\"üîó Document Chunk Linking:\")\n",
    "print(f\"Step 1 - Found relevant documents:\")\n",
    "for source in relevant_sources:\n",
    "    print(f\"- {source[:60]}...\")\n",
    "\n",
    "# Step 2: Get more chunks from these documents\n",
    "linked_retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": TOP_K,\n",
    "        \"filter\": Filter(\n",
    "            should=[  # OR condition across multiple files\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.source\",\n",
    "                    match=MatchText(text=source)\n",
    "                )\n",
    "                for source in relevant_sources\n",
    "            ]\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "linked_docs = linked_retriever.invoke(sample_query)\n",
    "print(f\"\\nStep 2 - Retrieved {len(linked_docs)} total chunks from relevant documents\")\n",
    "\n",
    "# Show distribution\n",
    "source_counts = Counter([doc.metadata['source'] for doc in linked_docs])\n",
    "for source, count in source_counts.items():\n",
    "    print(f\"- {count} chunks from {source[:50]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del initial_docs, relevant_sources, linked_retriever, linked_docs, source_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LLM-Guided Filtering \n",
    "\n",
    "**What it is:**  Use an LLM to classify queries and route to appropriate filtered retrievers.\n",
    "\n",
    "**When to use:**\n",
    "- Multi-domain knowledge bases\n",
    "- Complex query understanding needed\n",
    "- When simple keyword filtering isn't sufficient\n",
    "- Domain-specific optimization\n",
    "\n",
    "**‚úÖ Pros:** Intelligent routing, domain optimization  \n",
    "**‚ö†Ô∏è Cons:** Added LLM call overhead, potential classification errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing LLM-Guided Filtering:\n",
      "\n",
      "\n",
      "üìã Test 1: How does MC3G improve upon existing counterfactual explanation methods, particularly concerning cost computation and causal dependencies?\n",
      "redicted: Other | Expected: Other\n",
      "Correctly Classified ‚úÖ\n",
      "\n",
      "üìã Test 2: What are the key challenges in adapting existing open-vocabulary semantic segmentation (OVSS) frameworks, designed for natural images, to remote sensi\n",
      "redicted: Computer_Vision | Expected: Computer_Vision\n",
      "Correctly Classified ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# Setup LLM for query classification\n",
    "llm = model_manager.get_llm()\n",
    "\n",
    "TOPIC_IDENTIFICATION_TEMPLATE = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"\"\"\n",
    "Classify the query into one of two topics:\n",
    "\n",
    "- \"Computer_Vision\": if related to image/video processing, recognition, detection, segmentation, or OCR.  \n",
    "- \"Other\": for everything else.  \n",
    "\n",
    "Return only the class name.  \n",
    "\n",
    "Query: {query}  \n",
    "Topic:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def classify_query(query: str) -> str:\n",
    "    \"\"\"Classify query into Computer_Vision or Other.\"\"\"\n",
    "    category = llm.invoke(\n",
    "        TOPIC_IDENTIFICATION_TEMPLATE.format(query=query)\n",
    "    ).content.strip()\n",
    "    return category\n",
    "\n",
    "def get_retriever(category: str, k: int = TOP_K):\n",
    "    \"\"\"Return retriever based on classification.\"\"\"\n",
    "    if category == \"Computer_Vision\":\n",
    "        print(\"üéØ Using Computer Vision filtered retriever\\n\")\n",
    "        return vector_store.as_retriever(\n",
    "            search_kwargs={\n",
    "                \"k\": k,\n",
    "                \"filter\": Filter(\n",
    "                    must=[\n",
    "                        FieldCondition(\n",
    "                            key=\"metadata.source\", \n",
    "                            match=MatchText(text=\"Computer_Vision\")\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        print(\"üîç Using general retriever\\n\")\n",
    "        return vector_store.as_retriever(\n",
    "            search_kwargs={\"k\": k, \"score_threshold\": SCORE_THRESHOLD}\n",
    "        )\n",
    "\n",
    "# -------------------\n",
    "# Testing\n",
    "# -------------------\n",
    "print(\"üß™ Testing LLM-Guided Filtering:\\n\")\n",
    "llm_filtering_queries = [test_queries[i] for i in [0, 3]]  \n",
    "for i, query in enumerate(llm_filtering_queries, 1):\n",
    "    q_text = query[\"user_input\"]\n",
    "    expected = \"Computer_Vision\" if \"Computer_Vision\" in query[\"source_file\"] else \"Other\"\n",
    "\n",
    "    print(f\"\\nüìã Test {i}: {q_text[:150]}\")\n",
    "\n",
    "    predicted = classify_query(q_text)\n",
    "    print(f\"redicted: {predicted} | Expected: {expected}\")\n",
    "\n",
    "    result = \"Correctly Classified ‚úÖ\" if predicted == expected else \"Incorrectly Classified ‚ùå\"\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del llm, TOPIC_IDENTIFICATION_TEMPLATE, llm_filtering_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reranking\n",
    "\n",
    "**What it is:**  \n",
    "Applies a secondary model (e.g., cross-encoder, LLM, or relevance model) to reorder initial retrieval results, improving relevance ranking beyond similarity scores alone.  \n",
    "\n",
    "**When to use:**   \n",
    "- Dense or hybrid retrieval gives many candidates but order matters  \n",
    "- Need **higher precision** in the top results (e.g., top-3 for RAG context)  \n",
    "- Queries where subtle semantic nuances are important  \n",
    "\n",
    "**‚úÖ Pros:**   \n",
    "- Improves relevance of top results  \n",
    "- Reduces noise passed to the LLM  \n",
    "- Works well with hybrid/similarity search as a post-processing step  \n",
    "\n",
    "**‚ö†Ô∏è Cons:**   \n",
    "- Higher latency and compute cost  \n",
    "- Requires additional model training or fine-tuning for best results  \n",
    "- May not scale well for very large candidate sets  \n",
    "\n",
    "**Example use case:**  \n",
    "- Initial retriever returns 20 (>>K) research paper abstracts.  \n",
    "- A cross-encoder reranker re-scores them, surfacing the **most directly relevant Top-K** for the query context.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reranking\n",
    "<!-- ![Reranking](../utils/images/reranking.png) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer reranking.py \n",
    "def _setup_reranker_retriever():\n",
    "    \"\"\"Initialize the reranking retriever.\"\"\"\n",
    "    qdrant_path = config.QDRANT_DIR / strategy.value\n",
    "    qdrant_client = QdrantClient(path=str(qdrant_path))\n",
    "    embeddings = model_manager.get_embeddings()\n",
    "\n",
    "    vector_store = QdrantVectorStore(\n",
    "        client=qdrant_client,\n",
    "        collection_name=strategy.value,\n",
    "        embedding=embeddings,\n",
    "    )\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": top_k})\n",
    "\n",
    "    model = HuggingFaceCrossEncoder(model_name=constants.RERANKER_MODEL)\n",
    "    compressor = CrossEncoderReranker(model=model, top_n=top_n)\n",
    "    reranker_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor, base_retriever=retriever\n",
    "    )\n",
    "    return reranker_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reranker initialized\n",
      "Starting reranking evaluation...\n",
      "Evaluating 5 test queries\n",
      "\n",
      "Query 1: How does MC3G improve upon existing counterfactual explanation methods, particularly concerning cost...\n",
      "  Result 1: Reranker=0.923, Baseline=0.746\n",
      "  Result 2: Reranker=0.793, Baseline=0.690\n",
      "  Result 3: Reranker=0.657, Baseline=0.656\n",
      "  üìä Query avg: Reranker=0.791, Baseline=0.697\n",
      "\n",
      "Query 2: What limitations of the Copas-Jackson bound did the authors identify, and how did they address these...\n",
      "  Result 1: Reranker=0.852, Baseline=0.863\n",
      "  Result 2: Reranker=0.843, Baseline=0.803\n",
      "  Result 3: Reranker=0.863, Baseline=0.852\n",
      "  üìä Query avg: Reranker=0.853, Baseline=0.839\n",
      "\n",
      "Query 3: What is the key innovation of the proposed Riemannian change point detection method, and how does it...\n",
      "  Result 1: Reranker=0.664, Baseline=0.777\n",
      "  Result 2: Reranker=0.777, Baseline=0.634\n",
      "  Result 3: Reranker=0.915, Baseline=0.664\n",
      "  üìä Query avg: Reranker=0.785, Baseline=0.692\n",
      "\n",
      "Query 4: What are the key challenges in adapting existing open-vocabulary semantic segmentation (OVSS) framew...\n",
      "  Result 1: Reranker=0.843, Baseline=0.843\n",
      "  Result 2: Reranker=0.946, Baseline=0.946\n",
      "  Result 3: Reranker=0.781, Baseline=0.770\n",
      "  üìä Query avg: Reranker=0.857, Baseline=0.853\n",
      "\n",
      "Query 5: What are the key decidable restrictions on the SSM satisfiability problem identified in the paper, a...\n",
      "  Result 1: Reranker=0.889, Baseline=0.876\n",
      "  Result 2: Reranker=0.876, Baseline=0.863\n",
      "  Result 3: Reranker=0.853, Baseline=0.828\n",
      "  üìä Query avg: Reranker=0.873, Baseline=0.856\n",
      "\n",
      "==================================================\n",
      "üéØ FINAL RESULTS\n",
      "==================================================\n",
      "Reranker Average Score:  0.8317\n",
      "Baseline Average Score:  0.7874\n",
      "Improvement:             0.0443\n",
      "Improvement Percentage:  5.63%\n",
      "‚úÖ Reranking shows improvement!\n"
     ]
    }
   ],
   "source": [
    "from retrieval_playground.src.mid_retrieval.reranking import Reranker\n",
    "reranker = Reranker(qdrant_client=qdrant_client, top_n=TOP_K)\n",
    "reranker_evaluation_results = reranker.evaluate_reranking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del reranker, reranker_evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hybrid Filtering\n",
    "\n",
    "**What it is:**   \n",
    "Combines sparse retrieval (BM25/keyword-based) with dense retrieval (embeddings-based) to leverage both exact keyword matching and semantic similarity.\n",
    "\n",
    "**How it works:**  \n",
    "- BM25 ensures keyword precision (great for rare terms, acronyms, or exact matches).  \n",
    "- Dense retrieval ensures semantic recall (captures meaning even if words differ).  \n",
    "- Final results are combined (via weighted scores, reranking, or union).  \n",
    "\n",
    "**When to use:**  \n",
    "- Queries with a mix of rare keywords and semantic intent  \n",
    "- Domain-specific content with technical jargon (BM25 helps catch exact terms)  \n",
    "- General RAG pipelines where coverage + precision both matter  \n",
    "- When neither sparse nor dense alone gives consistently good results  \n",
    "\n",
    "**‚úÖ Pros:** Best of both worlds ‚Äì exact match + semantic understanding  \n",
    "**‚ö†Ô∏è Cons:** More complex to implement, higher compute cost  \n",
    "\n",
    "**Example use case:**  \n",
    "\n",
    "*Query:*  \n",
    "*‚ÄúWhat are the key challenges in adapting existing open-vocabulary semantic segmentation (OVSS) frameworks, designed for natural images, to remote sensing images, and how does SegEarth-OV address these challenges?‚Äù*  \n",
    "\n",
    "*BM25 (Sparse)*: Catches exact terms - *‚Äúsemantic segmentation‚Äù*, *‚Äúopen-vocabulary‚Äù*, *‚Äúremote sensing‚Äù*, *OVSS*  \n",
    "\n",
    "*Dense (Semantic)*: Finds paraphrases - *‚Äúpixel-level classification for satellite imagery‚Äù*, *‚Äúdomain adaptation from natural to aerial scenes‚Äù*, *‚Äúgeneralized segmentation across modalities‚Äù*  \n",
    "\n",
    "*Hybrid*: Returns both - keyword-heavy matches (*‚Äúopen-vocabulary segmentation for remote sensing‚Äù*) + semantic ones (*‚ÄúSegEarth-OV enables cross-domain satellite segmentation‚Äù*).  \n",
    "\n",
    "*Result:* Covers **precision (keywords)** + **recall (semantic similarity)**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid\n",
    "<!-- ![Hybrid Approach](../utils/images/hybrid_search.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about implementing hybrid approaches, check out [this article on hybrid search by Qdrant](https://qdrant.tech/articles/hybrid-search/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Method Comparison & Recommendations\n",
    "\n",
    "### Quick Comparison Table\n",
    "\n",
    "| Method | Best For | Pros | Cons | Complexity |\n",
    "|--------|----------|------|------|------------|\n",
    "| **Similarity Search** | General purpose | Fast, simple, reliable | May return duplicates | Low |\n",
    "| **MMR** | Diverse content | Reduces redundancy | May miss similar relevant content | Medium |\n",
    "| **Score Threshold** | Quality control | Ensures minimum quality | May return no results | Low |\n",
    "| **Dynamic Retrieval** | Variable content needs | Flexible, quality-focused | Unpredictable context length | Low |\n",
    "| **Metadata Filtering** | Domain-specific | Precise targeting | May miss cross-domain insights | Medium |\n",
    "| **Chunk Linking** | Document comprehension | Comprehensive coverage | Less relevant chunks included | Medium |\n",
    "| **LLM-Guided** | Multi-domain systems | Intelligent routing | LLM overhead, classification errors | High |\n",
    "| **Hybrid Retrieval (BM25 + Dense)** | Balanced semantic + keyword search | Captures both exact matches & semantic meaning | Requires tuning weight between BM25 & dense | High |\n",
    "| **Reranking** | Precision in top results | Improves top-k relevance, reduces noise | Higher latency & compute cost | High |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Recommendations\n",
    "\n",
    "#### **Start Simple**\n",
    "Begin with **Similarity Search + Score Threshold** for most applications.\n",
    "\n",
    "#### **Scale Up Based on Needs**\n",
    "- Add **MMR** if you notice repetitive results  \n",
    "- Use **Metadata Filtering** for multi-domain knowledge bases  \n",
    "- Implement **LLM-Guided Filtering** for complex routing needs  \n",
    "- Adopt **Hybrid Retrieval (BM25 + Dense)** when queries require both exact keyword matches (e.g., specific terms, codes) and semantic understanding (contextual intent)  \n",
    "- Add **Reranking** when you need **high precision in the top results** (e.g., top-3 context for LLMs)  \n",
    "\n",
    "#### **Fine-tuning Tips**\n",
    "- Adjust `score_threshold` based on your quality requirements (0.3‚Äì0.5 is typical)  \n",
    "- Use **Dynamic Retrieval** when context length flexibility is valuable  \n",
    "- Apply **Chunk Linking** for document-centric tasks  \n",
    "- Tune the BM25 vs Dense weighting in **Hybrid Retrieval** (common ranges: 0.3‚Äì0.7) depending on whether precision (keywords) or recall (semantics) is more important  \n",
    "- Use **Reranking** selectively on top-N candidates to balance cost vs accuracy  \n",
    "\n",
    "#### **Pro Tips**\n",
    "- Monitor retrieval metrics (precision, recall) to choose optimal methods  \n",
    "- Consider combining methods (e.g., MMR + Score Threshold, Hybrid + Reranking)  \n",
    "- Cache LLM classification results for repeated query patterns  \n",
    "- A/B test different approaches with your specific use case  \n",
    "- Start with simple methods and add complexity only when needed  \n",
    "\n",
    "#### **Common Patterns**\n",
    "- **High-quality RAG**: Similarity Search + Score Threshold + MMR  \n",
    "- **Multi-domain KB**: LLM-Guided Filtering + Metadata Filtering  \n",
    "- **Document Analysis**: Chunk Linking + Score Threshold  \n",
    "- **Exploratory Search**: Dynamic Retrieval + MMR  \n",
    "- **Balanced Search (keyword + meaning)**: Hybrid Retrieval (BM25 + Dense) + Score Threshold  \n",
    "- **High-precision Context**: Hybrid Retrieval + Reranking  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚úÖ Tutorial completed! You now know 9 different retrieval methods.\n",
    "Ready to build better RAG systems!\n",
    "\n",
    "Next steps:\n",
    "- Experiment with different methods on your own data\n",
    "- Combine methods for optimal results\n",
    "- Monitor and evaluate retrieval quality\n",
    "- Scale complexity based on your specific needs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydata_tutorial",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
