{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Methods Tutorial\n",
    "\n",
    "This notebook demonstrates various retrieval methods for RAG systems using vector databases. Each method serves different use cases and has specific advantages.\n",
    "\n",
    "## Overview of Methods\n",
    "\n",
    "1. **Basic Similarity Search** - Standard semantic search\n",
    "2. **MMR (Maximal Marginal Relevance)** - Balance relevance and diversity\n",
    "3. **Top-k Thresholding** - Quantity-based filtering\n",
    "4. **Adaptive/Dynamic Retrieval** - Flexible result counts\n",
    "5. **Metadata Filtering** - Context-aware search\n",
    "6. **Document Chunk Linking** - Multi-document retrieval\n",
    "7. **LLM-Guided Filtering** - Intelligent pre-filtering\n",
    "8. **Reranking** - Reorder results for higher precision  \n",
    "9. **Hybrid Retrieval** - Combine keyword and semantic search  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, Filter, FieldCondition, MatchValue, MatchText\n",
    "from retrieval_playground.utils import config, constants\n",
    "from retrieval_playground.utils.model_manager import model_manager\n",
    "from retrieval_playground.src.pre_retrieval.chunking_strategies import ChunkingStrategy\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import Dict, List, Any\n",
    "import json\n",
    "import gc\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test queries\n",
    "def load_test_queries() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load test queries from JSON file.\"\"\"\n",
    "    queries_path = config.TESTS_DIR / \"test_queries.json\"\n",
    "    with open(queries_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "test_queries = load_test_queries()\n",
    "sample_query = test_queries[0][\"user_input\"]\n",
    "print(f\"Sample query: {sample_query}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup vector database\n",
    "strategy = ChunkingStrategy.UNSTRUCTURED\n",
    "qdrant_client = QdrantClient(url=constants.QDRANT_URL, api_key=constants.QDRANT_KEY)\n",
    "embeddings = model_manager.get_embeddings()\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=strategy.value,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(\"Vector store initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters\n",
    "TOP_K = 3\n",
    "SCORE_THRESHOLD = 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Similarity Search\n",
    "\n",
    "**What it is:**  Standard cosine similarity between query and document embeddings.\n",
    "\n",
    "**When to use:**\n",
    "- Default choice for most RAG applications\n",
    "- When you need the most semantically similar content\n",
    "- Simple, fast, and reliable\n",
    "\n",
    "**‚úÖ Pros:** Fast, simple, works well for most cases  \n",
    "**‚ö†Ô∏è Cons:** May return very similar/duplicate content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic similarity search with scores\n",
    "context_docs_with_score = vector_store.similarity_search_with_relevance_scores(\n",
    "    sample_query, k=TOP_K\n",
    ")\n",
    "\n",
    "print(\"üìä Similarity Search Results:\")\n",
    "for i, (doc, score) in enumerate(context_docs_with_score, 1):\n",
    "    print(f\"{i}. Score: {score:.3f} | Source: {doc.metadata['source']} | ChunkID: {doc.metadata['chunk_id']} \")\n",
    "    print(f\"   Preview: {doc.page_content[:50].strip()}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MMR (Maximal Marginal Relevance)\n",
    "\n",
    "**What it is:**  Balances relevance with diversity to avoid near-duplicate results.\n",
    "\n",
    "**Formula:** `score = Œª * similarity(query, doc) ‚Äì (1 ‚Äì Œª) * max(similarity(doc, selected_docs))`\n",
    "\n",
    "**When to use:**\n",
    "- When documents have repetitive/similar content\n",
    "- Need diverse perspectives on the same topic\n",
    "- Quality over quantity approach\n",
    "\n",
    "**‚úÖ Pros:** Reduces redundancy, increases content diversity  \n",
    "**‚ö†Ô∏è Cons:** May miss highly relevant but similar content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMR search\n",
    "query_embedding = embeddings.embed_query(sample_query)\n",
    "mmr_docs_with_score = vector_store.max_marginal_relevance_search_with_score_by_vector(\n",
    "    embedding=query_embedding, k=TOP_K\n",
    ")\n",
    "\n",
    "print(\"MMR Search Results:\\n\")\n",
    "for i, (doc, score) in enumerate(mmr_docs_with_score, 1):\n",
    "    print(f\"{i}. MMR Score: {score:.3f} | Source: {doc.metadata['source']} | ChunkID: {doc.metadata['chunk_id']} \")\n",
    "    print(f\"   Preview: {doc.page_content[:50]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nComparison:\")\n",
    "print(\"Similarity sources:\", [doc[0].metadata['chunk_id'] for doc in context_docs_with_score])\n",
    "print(\"MMR sources:       \", [doc[0].metadata['chunk_id'] for doc in mmr_docs_with_score])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del context_docs_with_score, query_embedding, mmr_docs_with_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Top-k Retrieval with Thresholding (Quantity-Oriented)  \n",
    "\n",
    "**What it is:**  \n",
    "Return a fixed number of top-k results, optionally filtering out those below a minimum relevance score.  \n",
    "\n",
    "**When to use:**  \n",
    "- When you want a consistent number of results  \n",
    "- When coverage/quantity is more important than strict quality  \n",
    "- When downstream processing expects a predictable input size  \n",
    "\n",
    "**‚úÖ Pros:** Predictable result count, broader coverage  \n",
    "**‚ö†Ô∏è Cons:** May include less-relevant results if quality varies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score thresholding with fixed k\n",
    "context_docs_with_score = vector_store.similarity_search_with_relevance_scores(\n",
    "    sample_query, k=TOP_K\n",
    ")\n",
    "\n",
    "retriever_with_threshold = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": TOP_K, \"score_threshold\": SCORE_THRESHOLD}\n",
    ")\n",
    "threshold_docs = retriever_with_threshold.invoke(sample_query)\n",
    "\n",
    "print(f\"üìè Score Threshold Results (min score: {SCORE_THRESHOLD}):\")\n",
    "print(f\"Found {len(threshold_docs)} documents above threshold\")\n",
    "\n",
    "# Compare with scores\n",
    "all_scores = [score for _, score in context_docs_with_score]\n",
    "above_threshold = [score for score in all_scores if score >= SCORE_THRESHOLD]\n",
    "\n",
    "print(f\"All scores: {all_scores}\")\n",
    "print(f\"Above threshold ({SCORE_THRESHOLD}): {above_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score thresholding with fixed k -  With scores\n",
    "# Test with different queries to show variability\n",
    "print(\"\\nüìä Results for different queries:\")\n",
    "for i, query_data in enumerate(test_queries[:3], 1):\n",
    "    query = query_data[\"user_input\"]\n",
    "    docs =  vector_store.similarity_search_with_relevance_scores(query, k=TOP_K)\n",
    "    scores = [round(doc[1],2) for doc in docs]\n",
    "    print(f\"Query {i}: {len(docs)} results |  Scores: {scores} | Topic: {query[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del context_docs_with_score, retriever_with_threshold, threshold_docs, all_scores, above_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dynamic (Adaptive) Thresholding (Quality-Oriented)  \n",
    "\n",
    "**What it is:**  \n",
    "Return all results above a relevance score threshold, with no fixed k.  \n",
    "\n",
    "**When to use:**  \n",
    "- When ensuring only high-quality results is the priority  \n",
    "- When query difficulty varies widely  \n",
    "- When you can handle variable-length input for the LLM  \n",
    "\n",
    "**‚úÖ Pros:** Ensures higher-quality results, filters out noise  \n",
    "**‚ö†Ô∏è Cons:** Unpredictable result count, can increase costs for long contexts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic retrieval (no fixed k)\n",
    "dynamic_retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"score_threshold\": SCORE_THRESHOLD}\n",
    ")\n",
    "dynamic_docs = dynamic_retriever.invoke(sample_query)\n",
    "\n",
    "print(f\"üîÑ Dynamic Retrieval Results:\")\n",
    "print(f\"Found {len(dynamic_docs)} documents above threshold {SCORE_THRESHOLD}\")\n",
    "\n",
    "# Test with different queries to show variability\n",
    "print(\"\\nüìä Results for different queries:\")\n",
    "for i, query_data in enumerate(test_queries[:3], 1):\n",
    "    query = query_data[\"user_input\"]\n",
    "    docs = dynamic_retriever.invoke(query)\n",
    "    print(f\"Query {i}: {len(docs)} results | Topic: {query[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dynamic_retriever, dynamic_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metadata Filtering\n",
    "\n",
    "**What it is:**  Filter search results by document metadata (source, type, date, etc.).\n",
    "\n",
    "**When to use:**\n",
    "- Domain-specific searches (e.g., only medical papers)\n",
    "- Time-based filtering (recent documents only)\n",
    "- Source credibility filtering\n",
    "- User permission-based access control\n",
    "\n",
    "**‚úÖ Pros:** Precise targeting, context control  \n",
    "**‚ö†Ô∏è Cons:** May miss relevant content from filtered sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http import models as rest\n",
    "\n",
    "qdrant_client.create_payload_index(\n",
    "    collection_name=\"unstructured\",\n",
    "    field_name=\"metadata.source\",\n",
    "    field_schema=rest.PayloadSchemaType.TEXT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Exact source match\n",
    "target_file = \"Statistics_2025_Copas-Jackson-type_bounds_for_publication_bias_over_a_general_class_of___selecti.pdf\"\n",
    "\n",
    "exact_filter_retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": TOP_K,\n",
    "        \"filter\": Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.source\",  \n",
    "                    match=MatchText(text=target_file)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "exact_filtered_docs = exact_filter_retriever.invoke(sample_query)\n",
    "print(f\"üéØ Exact Source Filter Results:\")\n",
    "print(f\"Target: {target_file[:50]}...\")\n",
    "print(f\"Found {len(exact_filtered_docs)} results from this source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Substring/topic-based filtering\n",
    "topic_filter_retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": TOP_K,\n",
    "        \"filter\": Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.source\", \n",
    "                    match=MatchText(text=\"Statistics_2025\")  # substring match\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "topic_filtered_docs = topic_filter_retriever.invoke(sample_query)\n",
    "print(f\"\\nüìä Topic Filter Results (Statistics papers):\")\n",
    "print(f\"Found {len(topic_filtered_docs)} results\")\n",
    "for doc in topic_filtered_docs:\n",
    "    print(f\"- {doc.metadata['source'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del target_file, exact_filter_retriever, exact_filtered_docs, topic_filter_retriever, topic_filtered_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Document Chunk Linking\n",
    "\n",
    "**What it is:**  First find relevant documents, then retrieve more chunks from those same documents.\n",
    "\n",
    "**When to use:**\n",
    "- When relevant info might be spread across chunks in same document\n",
    "- Want comprehensive coverage of relevant documents\n",
    "- Building document-level understanding\n",
    "\n",
    "**‚úÖ Pros:** Comprehensive document coverage, maintains context  \n",
    "**‚ö†Ô∏è Cons:** May include less relevant chunks from relevant documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Find most relevant documents\n",
    "initial_docs = vector_store.similarity_search(sample_query, k=2)\n",
    "relevant_sources = [doc.metadata[\"source\"] for doc in initial_docs]\n",
    "\n",
    "print(f\"üîó Document Chunk Linking:\")\n",
    "print(f\"Step 1 - Found relevant documents:\")\n",
    "for source in relevant_sources:\n",
    "    print(f\"- {source[:60]}...\")\n",
    "\n",
    "# Step 2: Get more chunks from these documents\n",
    "linked_retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": TOP_K,\n",
    "        \"filter\": Filter(\n",
    "            should=[  # OR condition across multiple files\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.source\",\n",
    "                    match=MatchText(text=source)\n",
    "                )\n",
    "                for source in relevant_sources\n",
    "            ]\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "linked_docs = linked_retriever.invoke(sample_query)\n",
    "print(f\"\\nStep 2 - Retrieved {len(linked_docs)} total chunks from relevant documents\")\n",
    "\n",
    "# Show distribution\n",
    "source_counts = Counter([doc.metadata['source'] for doc in linked_docs])\n",
    "for source, count in source_counts.items():\n",
    "    print(f\"- {count} chunks from {source[:50]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del initial_docs, relevant_sources, linked_retriever, linked_docs, source_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LLM-Guided Filtering \n",
    "\n",
    "**What it is:**  Use an LLM to classify queries and route to appropriate filtered retrievers.\n",
    "\n",
    "**When to use:**\n",
    "- Multi-domain knowledge bases\n",
    "- Complex query understanding needed\n",
    "- When simple keyword filtering isn't sufficient\n",
    "- Domain-specific optimization\n",
    "\n",
    "**‚úÖ Pros:** Intelligent routing, domain optimization  \n",
    "**‚ö†Ô∏è Cons:** Added LLM call overhead, potential classification errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup LLM for query classification\n",
    "llm = model_manager.get_llm()\n",
    "\n",
    "TOPIC_IDENTIFICATION_TEMPLATE = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"\"\"\n",
    "Classify the query into one of two topics:\n",
    "\n",
    "- \"Computer_Vision\": if related to image/video processing, recognition, detection, segmentation, or OCR.  \n",
    "- \"Other\": for everything else.  \n",
    "\n",
    "Return only the class name.  \n",
    "\n",
    "Query: {query}  \n",
    "Topic:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def classify_query(query: str) -> str:\n",
    "    \"\"\"Classify query into Computer_Vision or Other.\"\"\"\n",
    "    category = llm.invoke(\n",
    "        TOPIC_IDENTIFICATION_TEMPLATE.format(query=query)\n",
    "    ).content.strip()\n",
    "    return category\n",
    "\n",
    "def get_retriever(category: str, k: int = TOP_K):\n",
    "    \"\"\"Return retriever based on classification.\"\"\"\n",
    "    if category == \"Computer_Vision\":\n",
    "        print(\"üéØ Using Computer Vision filtered retriever\\n\")\n",
    "        return vector_store.as_retriever(\n",
    "            search_kwargs={\n",
    "                \"k\": k,\n",
    "                \"filter\": Filter(\n",
    "                    must=[\n",
    "                        FieldCondition(\n",
    "                            key=\"metadata.source\", \n",
    "                            match=MatchText(text=\"Computer_Vision\")\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        print(\"üîç Using general retriever\\n\")\n",
    "        return vector_store.as_retriever(\n",
    "            search_kwargs={\"k\": k, \"score_threshold\": SCORE_THRESHOLD}\n",
    "        )\n",
    "\n",
    "# -------------------\n",
    "# Testing\n",
    "# -------------------\n",
    "print(\"üß™ Testing LLM-Guided Filtering:\\n\")\n",
    "llm_filtering_queries = [test_queries[i] for i in [0, 3]]  \n",
    "for i, query in enumerate(llm_filtering_queries, 1):\n",
    "    q_text = query[\"user_input\"]\n",
    "    expected = \"Computer_Vision\" if \"Computer_Vision\" in query[\"source_file\"] else \"Other\"\n",
    "\n",
    "    print(f\"\\nüìã Test {i}: {q_text[:150]}\")\n",
    "\n",
    "    predicted = classify_query(q_text)\n",
    "    print(f\"redicted: {predicted} | Expected: {expected}\")\n",
    "\n",
    "    result = \"Correctly Classified ‚úÖ\" if predicted == expected else \"Incorrectly Classified ‚ùå\"\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del llm, TOPIC_IDENTIFICATION_TEMPLATE, llm_filtering_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reranking\n",
    "\n",
    "**What it is:**  \n",
    "Applies a secondary model (e.g., cross-encoder, LLM, or relevance model) to reorder initial retrieval results, improving relevance ranking beyond similarity scores alone.  \n",
    "\n",
    "**When to use:**   \n",
    "- Dense or hybrid retrieval gives many candidates but order matters  \n",
    "- Need **higher precision** in the top results (e.g., top-3 for RAG context)  \n",
    "- Queries where subtle semantic nuances are important  \n",
    "\n",
    "**‚úÖ Pros:**   \n",
    "- Improves relevance of top results  \n",
    "- Reduces noise passed to the LLM  \n",
    "- Works well with hybrid/similarity search as a post-processing step  \n",
    "\n",
    "**‚ö†Ô∏è Cons:**   \n",
    "- Higher latency and compute cost  \n",
    "- Requires additional model training or fine-tuning for best results  \n",
    "- May not scale well for very large candidate sets  \n",
    "\n",
    "**Example use case:**  \n",
    "- Initial retriever returns 20 (>>K) research paper abstracts.  \n",
    "- A cross-encoder reranker re-scores them, surfacing the **most directly relevant Top-K** for the query context.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reranking\n",
    "<!-- ![Reranking](../utils/images/reranking.png) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer reranking.py \n",
    "def _setup_reranker_retriever():\n",
    "    \"\"\"Initialize the reranking retriever.\"\"\"\n",
    "    qdrant_path = config.QDRANT_DIR / strategy.value\n",
    "    qdrant_client = QdrantClient(path=str(qdrant_path))\n",
    "    embeddings = model_manager.get_embeddings()\n",
    "\n",
    "    vector_store = QdrantVectorStore(\n",
    "        client=qdrant_client,\n",
    "        collection_name=strategy.value,\n",
    "        embedding=embeddings,\n",
    "    )\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": top_k})\n",
    "\n",
    "    model = HuggingFaceCrossEncoder(model_name=constants.RERANKER_MODEL)\n",
    "    compressor = CrossEncoderReranker(model=model, top_n=top_n)\n",
    "    reranker_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor, base_retriever=retriever\n",
    "    )\n",
    "    return reranker_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval_playground.src.mid_retrieval.reranking import Reranker\n",
    "reranker = Reranker(qdrant_client=qdrant_client, top_n=TOP_K)\n",
    "reranker_evaluation_results = reranker.evaluate_reranking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del reranker, reranker_evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hybrid Filtering\n",
    "\n",
    "**What it is:**   \n",
    "Combines sparse retrieval (BM25/keyword-based) with dense retrieval (embeddings-based) to leverage both exact keyword matching and semantic similarity.\n",
    "\n",
    "**How it works:**  \n",
    "- BM25 ensures keyword precision (great for rare terms, acronyms, or exact matches).  \n",
    "- Dense retrieval ensures semantic recall (captures meaning even if words differ).  \n",
    "- Final results are combined (via weighted scores, reranking, or union).  \n",
    "\n",
    "**When to use:**  \n",
    "- Queries with a mix of rare keywords and semantic intent  \n",
    "- Domain-specific content with technical jargon (BM25 helps catch exact terms)  \n",
    "- General RAG pipelines where coverage + precision both matter  \n",
    "- When neither sparse nor dense alone gives consistently good results  \n",
    "\n",
    "**‚úÖ Pros:** Best of both worlds ‚Äì exact match + semantic understanding  \n",
    "**‚ö†Ô∏è Cons:** More complex to implement, higher compute cost  \n",
    "\n",
    "**Example use case:**  \n",
    "\n",
    "*Query:*  \n",
    "*‚ÄúWhat are the key challenges in adapting existing open-vocabulary semantic segmentation (OVSS) frameworks, designed for natural images, to remote sensing images, and how does SegEarth-OV address these challenges?‚Äù*  \n",
    "\n",
    "*BM25 (Sparse)*: Catches exact terms - *‚Äúsemantic segmentation‚Äù*, *‚Äúopen-vocabulary‚Äù*, *‚Äúremote sensing‚Äù*, *OVSS*  \n",
    "\n",
    "*Dense (Semantic)*: Finds paraphrases - *‚Äúpixel-level classification for satellite imagery‚Äù*, *‚Äúdomain adaptation from natural to aerial scenes‚Äù*, *‚Äúgeneralized segmentation across modalities‚Äù*  \n",
    "\n",
    "*Hybrid*: Returns both - keyword-heavy matches (*‚Äúopen-vocabulary segmentation for remote sensing‚Äù*) + semantic ones (*‚ÄúSegEarth-OV enables cross-domain satellite segmentation‚Äù*).  \n",
    "\n",
    "*Result:* Covers **precision (keywords)** + **recall (semantic similarity)**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid\n",
    "<!-- ![Hybrid Approach](../utils/images/hybrid_search.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about implementing hybrid approaches, check out [this article on hybrid search by Qdrant](https://qdrant.tech/articles/hybrid-search/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Method Comparison & Recommendations\n",
    "\n",
    "### Quick Comparison Table\n",
    "\n",
    "| Method | Best For | Pros | Cons | Complexity |\n",
    "|--------|----------|------|------|------------|\n",
    "| **Similarity Search** | General purpose | Fast, simple, reliable | May return duplicates | Low |\n",
    "| **MMR** | Diverse content | Reduces redundancy | May miss similar relevant content | Medium |\n",
    "| **Score Threshold** | Quality control | Ensures minimum quality | May return no results | Low |\n",
    "| **Dynamic Retrieval** | Variable content needs | Flexible, quality-focused | Unpredictable context length | Low |\n",
    "| **Metadata Filtering** | Domain-specific | Precise targeting | May miss cross-domain insights | Medium |\n",
    "| **Chunk Linking** | Document comprehension | Comprehensive coverage | Less relevant chunks included | Medium |\n",
    "| **LLM-Guided** | Multi-domain systems | Intelligent routing | LLM overhead, classification errors | High |\n",
    "| **Hybrid Retrieval (BM25 + Dense)** | Balanced semantic + keyword search | Captures both exact matches & semantic meaning | Requires tuning weight between BM25 & dense | High |\n",
    "| **Reranking** | Precision in top results | Improves top-k relevance, reduces noise | Higher latency & compute cost | High |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Recommendations\n",
    "\n",
    "#### **Start Simple**\n",
    "Begin with **Similarity Search + Score Threshold** for most applications.\n",
    "\n",
    "#### **Scale Up Based on Needs**\n",
    "- Add **MMR** if you notice repetitive results  \n",
    "- Use **Metadata Filtering** for multi-domain knowledge bases  \n",
    "- Implement **LLM-Guided Filtering** for complex routing needs  \n",
    "- Adopt **Hybrid Retrieval (BM25 + Dense)** when queries require both exact keyword matches (e.g., specific terms, codes) and semantic understanding (contextual intent)  \n",
    "- Add **Reranking** when you need **high precision in the top results** (e.g., top-3 context for LLMs)  \n",
    "\n",
    "#### **Fine-tuning Tips**\n",
    "- Adjust `score_threshold` based on your quality requirements (0.3‚Äì0.5 is typical)  \n",
    "- Use **Dynamic Retrieval** when context length flexibility is valuable  \n",
    "- Apply **Chunk Linking** for document-centric tasks  \n",
    "- Tune the BM25 vs Dense weighting in **Hybrid Retrieval** (common ranges: 0.3‚Äì0.7) depending on whether precision (keywords) or recall (semantics) is more important  \n",
    "- Use **Reranking** selectively on top-N candidates to balance cost vs accuracy  \n",
    "\n",
    "#### **Pro Tips**\n",
    "- Monitor retrieval metrics (precision, recall) to choose optimal methods  \n",
    "- Consider combining methods (e.g., MMR + Score Threshold, Hybrid + Reranking)  \n",
    "- Cache LLM classification results for repeated query patterns  \n",
    "- A/B test different approaches with your specific use case  \n",
    "- Start with simple methods and add complexity only when needed  \n",
    "\n",
    "#### **Common Patterns**\n",
    "- **High-quality RAG**: Similarity Search + Score Threshold + MMR  \n",
    "- **Multi-domain KB**: LLM-Guided Filtering + Metadata Filtering  \n",
    "- **Document Analysis**: Chunk Linking + Score Threshold  \n",
    "- **Exploratory Search**: Dynamic Retrieval + MMR  \n",
    "- **Balanced Search (keyword + meaning)**: Hybrid Retrieval (BM25 + Dense) + Score Threshold  \n",
    "- **High-precision Context**: Hybrid Retrieval + Reranking  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚úÖ Tutorial completed! You now know 9 different retrieval methods.\n",
    "Ready to build better RAG systems!\n",
    "\n",
    "Next steps:\n",
    "- Experiment with different methods on your own data\n",
    "- Combine methods for optimal results\n",
    "- Monitor and evaluate retrieval quality\n",
    "- Scale complexity based on your specific needs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydata_tutorial",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
