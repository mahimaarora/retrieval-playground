{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Chain Methods for RAG: A Comprehensive Tutorial\n",
    "\n",
    "This notebook demonstrates different methods for combining retrieved documents to generate answers in RAG systems. Each method has its own strengths and use cases.\n",
    "\n",
    "## Overview of Methods\n",
    "\n",
    "1. **Stuff Documents Chain** - Simple concatenation of all documents  \n",
    "2. **Refine Documents Chain** - Iterative refinement of answers  \n",
    "3. **Map-Rerank Chain** - Score and rank individual document answers  \n",
    "4. **Map-Reduce Chain** - Summarize then combine approach  \n",
    "\n",
    "For detailed implementation code of document chain strategies, refer to:\n",
    "\n",
    "**`retrieval_playground/src/post_retrieval/document_chain.py`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import warnings\n",
    "from typing import Dict, List, Any\n",
    "from IPython.display import Image, display\n",
    "import gc\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import pandas as pd\n",
    "from retrieval_playground.utils import config\n",
    "from retrieval_playground.src.baseline_rag import RAG\n",
    "from retrieval_playground.src.evaluation import RAGEvaluator\n",
    "from retrieval_playground.src.pre_retrieval.chunking_strategies import ChunkingStrategy\n",
    "from retrieval_playground.src.post_retrieval import document_chain\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data and Initialize Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_queries() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load test queries from JSON file.\"\"\"\n",
    "    queries_path = config.TESTS_DIR / \"test_queries.json\"\n",
    "    with open(queries_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Initialize RAG system and evaluator\n",
    "strategy = ChunkingStrategy.UNSTRUCTURED\n",
    "rag = RAG(strategy=strategy)\n",
    "evaluator = RAGEvaluator(metrics=['faithfulness', 'answer_relevancy'])\n",
    "\n",
    "# Load test queries (using 1 for demo)\n",
    "test_queries = load_test_queries()[1:2]\n",
    "ground_truths = [q[\"reference\"] for q in test_queries]\n",
    "\n",
    "print(f\"Sample query: {test_queries[0]['user_input']}\")\n",
    "print(f\"Number of test queries: {len(test_queries)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Helper Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_method(method_name: str, rag_results: List[Dict], ground_truths: List[str]):\n",
    "    \"\"\"Helper function to evaluate and display results for any method.\"\"\"\n",
    "    print(f\"\\n=== {method_name} Results ===\")\n",
    "    \n",
    "    # Evaluate with RAGAS metrics\n",
    "    scores = evaluator.evaluate_rag_results(rag_results, ground_truths)\n",
    "    \n",
    "    # Display metrics\n",
    "    print(f\"Average Faithfulness: {np.round(np.nanmean(scores['faithfulness']), 2)}\")\n",
    "    print(f\"Average Answer Relevancy: {np.round(np.nanmean(scores['answer_relevancy']), 2)}\")\n",
    "    \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Stuff Documents Chain\n",
    "\n",
    "**What it is:** Simply concatenates all retrieved documents and passes them to the LLM in a single prompt.\n",
    "\n",
    "**When to use:**\n",
    "- Small number of documents that fit within context window\n",
    "- When you want the LLM to consider all information simultaneously\n",
    "- Fastest and simplest approach\n",
    "\n",
    "**✅ Pros:** Simple, fast, considers all context at once <br>\n",
    "**⚠️ Cons:** Limited by context window, may overwhelm LLM with too much information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Refine Chain](../utils/images/stuff.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional LangChain approach\n",
    "stuff_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
    "    Please provide a comprehensive answer based on the context below. \n",
    "    If the context doesn't contain enough information to answer the question, please say so.\n",
    "\n",
    "    Question: {question} \n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Answer:\"\"\"\n",
    ")\n",
    "\n",
    "stuff_chain = document_chain.setup_stuff_chain()\n",
    "\n",
    "# Test the method\n",
    "stuff_results = []\n",
    "for test_query in test_queries:\n",
    "    query = test_query[\"user_input\"]\n",
    "    docs = [doc[0] for doc in rag.retrieve_context(query)]\n",
    "    result = stuff_chain.invoke({\"context\": docs, \"question\": query})\n",
    "    stuff_results.append({\n",
    "        \"question\": query, \n",
    "        \"answer\": result, \n",
    "        \"context\": [{\"content\": doc.page_content} for doc in docs]\n",
    "    })\n",
    "\n",
    "# Evaluate\n",
    "stuff_scores = evaluate_method(\"Stuff Documents Chain\", stuff_results, ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del stuff_prompt, stuff_results, stuff_chain\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Refine Documents Chain\n",
    "\n",
    "**What it is:** Processes documents sequentially, starting with an initial answer from the first document, then refining it with each subsequent document.\n",
    "\n",
    "**When to use:**\n",
    "- When you have many documents that don't fit in context window\n",
    "- When you want iterative improvement of answers\n",
    "- When document order matters\n",
    "\n",
    "**✅ Pros:** Handles large document sets, iterative refinement, maintains context <br>\n",
    "**⚠️ Cons:** Slower (multiple LLM calls), order-dependent, potential information loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Refine Chain](../utils/images/refine.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization prompt\n",
    "summarize_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \n",
    "    \"\"\"You are a helpful assistant that answers questions based on the given context.\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Provide the best possible answer based on this context:\"\"\")\n",
    "])\n",
    "\n",
    "\n",
    "# Refinement prompt\n",
    "refine_template = \"\"\"\n",
    "We have an existing answer so far:\n",
    "{existing_answer}\n",
    "\n",
    "Here is some new context:\n",
    "------------\n",
    "{context}\n",
    "------------\n",
    "\n",
    "Refine the existing answer where needed, keeping it accurate and comprehensive.\n",
    "If the new context is not useful, keep the answer unchanged.\n",
    "\"\"\"\n",
    "\n",
    "refine_chain = document_chain.setup_refine_chain()\n",
    "\n",
    "# Test the method\n",
    "refine_results = []\n",
    "for test_query in test_queries:\n",
    "    query = test_query[\"user_input\"]\n",
    "    docs = [doc[0] for doc in rag.retrieve_context(query)]\n",
    "    result = refine_chain.invoke({\"input_documents\": docs, \"question\": query})[\"output_text\"]\n",
    "    refine_results.append({\n",
    "        \"question\": query, \n",
    "        \"answer\": result, \n",
    "        \"context\": [{\"content\": doc.page_content} for doc in docs]\n",
    "    })\n",
    "# Evaluate\n",
    "refine_scores = evaluate_method(\"Refine Documents Chain\", refine_results, ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del summarize_prompt, refine_template, refine_results, refine_chain\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Map-Rerank Chain\n",
    "\n",
    "**What it is:** Processes each document independently to generate answers with confidence scores, then selects the highest-scoring answer.\n",
    "\n",
    "**When to use:**\n",
    "- When you want to identify the most relevant document\n",
    "- When documents might contain conflicting information\n",
    "- When you need confidence scores for answers\n",
    "\n",
    "**✅ Pros:** Parallel processing, confidence scoring, handles conflicting information <br>\n",
    "**⚠️ Cons:** Only uses one document's information, may miss connections between documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Map-Rerank Chain](../utils/images/map_rerank.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    You are a helpful assistant. \n",
    "    Answer the following question using ONLY the given context. \n",
    "    If the context does not contain the answer, say \"Not enough information.\"\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Provide your answer and a confidence score (1-10) in this format:\n",
    "    <Answer>\n",
    "    Score: <Score>\n",
    "    \"\"\"\n",
    "\n",
    "rerank_chain = document_chain.setup_map_rerank_chain()\n",
    "\n",
    "# Test the method\n",
    "rerank_results = []\n",
    "for test_query in test_queries:\n",
    "    query = test_query[\"user_input\"]\n",
    "    docs = [doc[0] for doc in rag.retrieve_context(query)]\n",
    "    result = rerank_chain.invoke({\"input_documents\": docs, \"question\": query})[\"output_text\"]\n",
    "    rerank_results.append({\n",
    "        \"question\": query, \n",
    "        \"answer\": result, \n",
    "        \"context\": [{\"content\": doc.page_content} for doc in docs]\n",
    "    })\n",
    "\n",
    "# Evaluate\n",
    "rerank_scores = evaluate_method(\"Map-Rerank Chain\", rerank_results, ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del prompt_template, rerank_results, rerank_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4: Map-Reduce Chain\n",
    "\n",
    "**What it is:** First maps (summarizes) each document independently, then reduces (combines) all summaries to generate a final answer.\n",
    "\n",
    "**When to use:**\n",
    "- Large number of documents that exceed context window\n",
    "- When you want to consider information from all documents\n",
    "- When documents contain complementary information\n",
    "\n",
    "**✅ Pros:** Handles large document sets, parallel processing, considers all information <br>\n",
    "**⚠️ Cons:** Two-step process (slower), potential information loss in summarization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Map-Reduce Chain](../utils/images/map_reduce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map prompt - summarizes each document\n",
    "map_template = \"Write a concise summary of the following: {docs}\"\n",
    "\n",
    "reduce_template = \"\"\"The following is a set of summaries:\n",
    "{docs}\n",
    "\n",
    "Based on these summaries, answer the question: {question}\"\"\"\n",
    "\n",
    "map_reduce_chain = document_chain.setup_map_reduce_chain()\n",
    "\n",
    "# Test the method\n",
    "map_reduce_results = []\n",
    "for test_query in test_queries:\n",
    "    query = test_query[\"user_input\"]\n",
    "    docs = [doc[0] for doc in rag.retrieve_context(query)]\n",
    "    result = map_reduce_chain.invoke({\"input_documents\": docs, \"question\": query})[\"output_text\"]\n",
    "    map_reduce_results.append({\n",
    "        \"question\": query, \n",
    "        \"answer\": result, \n",
    "        \"context\": [{\"content\": doc.page_content} for doc in docs]\n",
    "    })\n",
    "\n",
    "# Evaluate\n",
    "map_reduce_scores = evaluate_method(\"Map-Reduce Chain\", map_reduce_results, ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del map_template, reduce_template, docs, result, map_reduce_results, map_reduce_chain\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method Comparison and Summary\n",
    "\n",
    "Let's compare all methods side by side:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Method': ['Stuff Chain', 'Refine Chain', 'Map-Rerank Chain', 'Map-Reduce Chain'],\n",
    "    'Faithfulness': [\n",
    "        np.round(np.nanmean(stuff_scores['faithfulness']), 2),\n",
    "        np.round(np.nanmean(refine_scores['faithfulness']), 2),\n",
    "        np.round(np.nanmean(rerank_scores['faithfulness']), 2),\n",
    "        np.round(np.nanmean(map_reduce_scores['faithfulness']), 2)\n",
    "    ],\n",
    "    'Answer Relevancy': [\n",
    "        np.round(np.nanmean(stuff_scores['answer_relevancy']), 2),\n",
    "        np.round(np.nanmean(refine_scores['answer_relevancy']), 2),\n",
    "        np.round(np.nanmean(rerank_scores['answer_relevancy']), 2),\n",
    "        np.round(np.nanmean(map_reduce_scores['answer_relevancy']), 2)\n",
    "    ],\n",
    "    'Best For': [\n",
    "        'Small doc sets, simple queries',\n",
    "        'Large doc sets, iterative refinement',\n",
    "        'Confidence scoring, conflicting info',\n",
    "        'Large doc sets, comprehensive answers'\n",
    "    ],\n",
    "    'Speed': ['Fastest', 'Slow', 'Medium', 'Slow'],\n",
    "    'Context Window': ['Limited', 'Unlimited', 'Limited per doc', 'Unlimited']\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n=== Method Comparison ===\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph Implementation\n",
    "\n",
    "```python \n",
    "app = document_chain.setup_refine_chain_langgraph()\n",
    "# app = document_chain.setup_map_rerank_chain_langgraph()\n",
    "# app = document_chain.setup_map_reduce_chain_langgraph\n",
    "\n",
    "# --- Option 1: Render graph directly in notebook ---\n",
    "try:\n",
    "    display(\n",
    "        Image(app.get_graph().draw_mermaid_png())\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Graph rendering failed. Try Option 2 below.\")\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "# --- Option 2: Get Mermaid code for manual rendering ---\n",
    "# If Option 1 fails, uncomment the lines below.\n",
    "# Copy the printed code into https://mermaid.live/ to view your graph.\n",
    "\n",
    "# mermaid_code = app.get_graph().draw_mermaid()\n",
    "# print(mermaid_code)\n",
    "```\n",
    "\n",
    "<!-- ![Graph](../utils/images/graph_refine.png) -->\n",
    "\n",
    "```python\n",
    "============================================================\n",
    "Test LangGraph implementation\n",
    "============================================================\n",
    "\n",
    "# Print header for clarity\n",
    "print(\"\\n=== Testing LangGraph Implementation ===\")\n",
    "\n",
    "# Take a sample query (first test case input)\n",
    "query = test_queries[0][\"user_input\"]\n",
    "\n",
    "# Retrieve relevant documents for the given query\n",
    "docs = [doc[0].page_content for doc in rag.retrieve_context(query)]\n",
    "\n",
    "# Run LangGraph refine app with query and retrieved docs\n",
    "final_state = app.invoke({\"question\": query, \"docs\": docs, \"index\": 0})\n",
    "\n",
    "# Display only the first 200 characters of the answer for readability\n",
    "print(f\"LangGraph Result: {final_state['answer'][:200]}...\") \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📌 Recommendations  \n",
    "\n",
    "### Choose Your Method Based On:  \n",
    "\n",
    "1. **Document Count & Size**  \n",
    "   - Few small documents → **Stuff Chain**  \n",
    "   - Many documents → **Refine Chain** or **Map-Reduce Chain**  \n",
    "\n",
    "2. **Information Quality**  \n",
    "   - Conflicting information → **Map-Rerank Chain**  \n",
    "   - Complementary information → **Map-Reduce Chain**  \n",
    "\n",
    "3. **Performance Requirements**  \n",
    "   - Speed critical → **Stuff Chain**  \n",
    "   - Quality critical → **Refine Chain** or **Map-Reduce Chain**  \n",
    "\n",
    "4. **Special Needs**  \n",
    "   - Need confidence scores → **Map-Rerank Chain**  \n",
    "   - Need iterative improvement → **Refine Chain**  \n",
    "   - Need comprehensive coverage → **Map-Reduce Chain**  \n",
    "\n",
    "### Modern vs Traditional Approaches  \n",
    "\n",
    "- **LangGraph**: Best for complex workflows, debugging, and fine-grained control  \n",
    "- **Traditional LangChain**: Easier for standard use cases, minimal setup required  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Experiment with Your Own Data\n",
    "\n",
    "Use the code below to test different methods with your own queries:\n",
    "\n",
    "```python\n",
    "def test_all_methods(query: str, show_context: bool = False):\n",
    "    \"\"\"Test all methods with a custom query.\"\"\"\n",
    "    print(f\"\\n=== Testing Query: {query} ===\")\n",
    "    \n",
    "    # Get documents\n",
    "    docs = [doc[0] for doc in rag.retrieve_context(query)]\n",
    "    \n",
    "    if show_context:\n",
    "        print(f\"\\nRetrieved {len(docs)} documents\")\n",
    "        for i, doc in enumerate(docs[:2]):  # Show first 2\n",
    "            print(f\"Doc {i+1}: {doc.page_content[:100]}...\")\n",
    "    \n",
    "    # Test each method\n",
    "    methods = {\n",
    "        \"Stuff Chain\": lambda: stuff_chain.invoke({\"context\": docs, \"question\": query}),\n",
    "        \"Refine Chain\": lambda: refine_chain.invoke({\"input_documents\": docs, \"question\": query})[\"output_text\"],\n",
    "        \"Map-Rerank Chain\": lambda: rerank_chain.invoke({\"input_documents\": docs, \"question\": query})[\"output_text\"],\n",
    "        \"Map-Reduce Chain\": lambda: map_reduce_chain.invoke({\"input_documents\": docs, \"question\": query})[\"output_text\"]\n",
    "    }\n",
    "    \n",
    "    for method_name, method_func in methods.items():\n",
    "        try:\n",
    "            result = method_func()\n",
    "            print(f\"\\n{method_name}: {result[:200]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n{method_name}: Error - {str(e)}\")\n",
    "\n",
    "# Example usage - uncomment to test\n",
    "test_all_methods(\"What is the key innovation of the proposed Riemannian change point detection method?\", show_context=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial demonstrated four different approaches to combining documents in RAG systems:\n",
    "\n",
    "1. **Stuff Documents Chain** - Simple and fast for small document sets  \n",
    "2. **Refine Documents Chain** - Iterative improvement for large document sets  \n",
    "3. **Map-Rerank Chain** - Confidence-based selection from individual documents  \n",
    "4. **Map-Reduce Chain** - Comprehensive approach for large, complementary document sets  \n",
    "\n",
    "Each method has its strengths and is suitable for different scenarios. The choice depends on your specific requirements for speed, accuracy, document size, and information quality.\n",
    "\n",
    "**Key Takeaways:**  \n",
    "- **Stuff Chain**: Best for simple cases with few documents  \n",
    "- **Refine Chain**: Best for iterative improvement with many documents  \n",
    "- **Map-Rerank**: Best when you need confidence scores and document ranking  \n",
    "- **Map-Reduce**: Best for comprehensive analysis of large document sets  \n",
    "- **LangGraph**: Provides better control and debugging capabilities  \n",
    "- **Traditional LangChain**: Simpler setup for standard use cases  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images Reference: https://mlpills.substack.com/p/issue-71-langchains-text-processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "og",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
