{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Retrieval Methods: Comprehensive Guide to Document Chunking Strategies\n",
    "\n",
    "This notebook provides a comprehensive comparison of document chunking strategies for Retrieval-Augmented Generation (RAG) systems. Document chunking is a critical preprocessing step that directly impacts the quality of information retrieval and subsequent answer generation.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "### Four Core Document Chunking Strategies\n",
    "1. **Baseline Chunking**: Character-based splitting with fixed size limits\n",
    "2. **Recursive Character Chunking**: Hierarchical text splitting that respects natural language boundaries\n",
    "3. **Unstructured Chunking**: Structure-aware document processing that preserves semantic elements\n",
    "4. **Docling Chunking**: Advanced hybrid parsing with sophisticated document understanding\n",
    "\n",
    "### Evaluation Framework\n",
    "- Systematic evaluation methodology for chunking strategies\n",
    "- Key performance metrics for RAG system assessment\n",
    "\n",
    "## Dataset and Methodology\n",
    "\n",
    "We demonstrate these techniques using research papers from the [mahimaarora025/research_papers](https://huggingface.co/datasets/mahimaarora025/research_papers/tree/main/sample_research_papers) dataset. This dataset contains peer-reviewed academic papers spanning multiple domains:\n",
    "- Analytics and Data Science\n",
    "- Computer Vision\n",
    "- Generative AI\n",
    "- Machine Learning\n",
    "- Statistics\n",
    "\n",
    "The diverse academic content provides an excellent testbed for evaluating chunking strategies across different document structures and content types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup and Dependencies\n",
    "\n",
    "This section initializes the required libraries and configures the environment for our chunking strategy comparison. We'll use a combination of document processing libraries, language models, and evaluation frameworks.\n",
    "\n",
    "### Key Dependencies\n",
    "- **LangChain**: Document loading and text splitting utilities\n",
    "- **Unstructured**: Advanced document parsing and structure recognition\n",
    "- **Docling**: State-of-the-art document conversion and chunking\n",
    "- **HuggingFace Transformers**: Embedding models for semantic similarity\n",
    "- **Qdrant**: Vector database for storing and retrieving document chunks\n",
    "- **RAGAS**: Evaluation framework for RAG system assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python libraries for file handling and data manipulation\n",
    "import os\n",
    "import gc\n",
    "import tempfile\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "# Dataset loading from HuggingFace Hub\n",
    "from datasets import load_dataset\n",
    "\n",
    "# LangChain ecosystem for document processing and language model integration\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Advanced document processing libraries\n",
    "from unstructured.partition.pdf import partition_pdf  # PDF parsing with structure recognition\n",
    "from unstructured.chunking.title import chunk_by_title  # Title-based intelligent chunking\n",
    "from docling.document_converter import DocumentConverter  # Advanced document conversion\n",
    "from docling.chunking import HybridChunker  # Hybrid semantic-syntactic chunking\n",
    "\n",
    "# Vector database and embeddings for semantic search\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "# Data analysis and evaluation frameworks\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from retrieval_playground.src.pre_retrieval.chunking_evaluation import ChunkingEvaluator\n",
    "\n",
    "# Additional Docling components for markdown processing\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_docling.loader import ExportType\n",
    "from langchain_docling import DoclingLoader\n",
    "\n",
    "# Environment variable management\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# System configuration and API keys\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")  # Gemini API for language model operations\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")  # Qdrant cloud instance URL\n",
    "QDRANT_KEY = os.getenv(\"QDRANT_KEY\")  # Qdrant authentication key\n",
    "EMBEDDING_MODEL = \"Qwen/Qwen3-Embedding-0.6B\"  # Lightweight multilingual embedding model\n",
    "\n",
    "print(\"Imports completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify API credentials are available\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"Please set GOOGLE_API_KEY environment variable\")\n",
    "\n",
    "# Initialize Google Gemini language model for text generation and evaluation\n",
    "# Using Gemini 2.0 Flash for fast inference with low temperature for consistency\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.1,  # Low temperature for deterministic outputs\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "# Initialize HuggingFace embedding model for semantic similarity computation\n",
    "# Qwen3-Embedding-0.6B provides efficient multilingual embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL\n",
    ")\n",
    "\n",
    "print(\"Models initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del llm, embeddings\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loading and Preprocessing\n",
    "\n",
    "This section demonstrates loading a research paper to serve as our test document for comparing chunking strategies. We'll use a representative academic paper that contains typical document structures found in research literature.\n",
    "\n",
    "### Data Source\n",
    "The test document comes from the research papers collection available at:\n",
    "https://huggingface.co/datasets/mahimaarora025/research_papers/tree/main/sample_research_papers\n",
    "\n",
    "To get started -\n",
    "1. Go to the above url\n",
    "2. Download the paper with title - `Generative_AI_2025_Frozen_in_Time__Parameter-Efficient_Time_Series_Transformers_via___Reservoir-Ind.pdf`\n",
    "3. Create a new folder inside data --> `data/sample_research_papers`.\n",
    "4. Move the downloaded file to the above folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"../data/sample_research_papers/Generative_AI_2025_Frozen_in_Time__Parameter-Efficient_Time_Series_Transformers_via___Reservoir-Ind.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Initialize PDF loader for the selected research paper\n",
    "loader = PyPDFLoader(str(pdf_path))\n",
    "pdf_docs = loader.load()  # Load all pages as separate document objects\n",
    "\n",
    "# Extract text content from all pages\n",
    "sample_data = []\n",
    "for i in range(len(pdf_docs)):\n",
    "    sample_data.append(pdf_docs[i].page_content)\n",
    "\n",
    "# Combine all pages into a single text document with page breaks preserved\n",
    "sample_data = '\\n\\n'.join(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Chunking Strategy Implementations\n",
    "\n",
    "This section provides hands-on demonstrations of four distinct chunking approaches. Each strategy represents a different philosophy for dividing documents into manageable pieces while preserving semantic coherence and structural integrity.\n",
    "\n",
    "## Strategy 1: Baseline Character-Based Chunking\n",
    "\n",
    "### Overview\n",
    "The baseline approach uses simple character counting to divide text into fixed-size chunks. This method prioritizes speed and simplicity over semantic preservation.\n",
    "\n",
    "### Characteristics\n",
    "- **Speed**: Fastest execution time\n",
    "- **Simplicity**: Minimal configuration required\n",
    "- **Limitations**: May split sentences, paragraphs, or concepts mid-way\n",
    "- **Best Use Cases**: Large-scale processing where speed trumps precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configure baseline character-based text splitter\n",
    "# This approach splits text based on character count with minimal intelligence\n",
    "baseline_splitter = CharacterTextSplitter(\n",
    "    chunk_size=5000,        # Target chunk size in characters\n",
    "    chunk_overlap=100,      # Character overlap between consecutive chunks\n",
    "    separator=\"\\n\\n\"        # Preferred split point (paragraph breaks)\n",
    ")\n",
    "\n",
    "# Apply baseline chunking to the sample document\n",
    "baseline_chunks = baseline_splitter.split_text(sample_data)\n",
    "\n",
    "# Display results summary\n",
    "print(\"BASELINE CHUNKING RESULTS\")\n",
    "print(f\"Number of chunks: {len(baseline_chunks)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Show first chunk as example\n",
    "print(baseline_chunks[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 2: Recursive Character Chunking\n",
    "\n",
    "### Overview\n",
    "Recursive character chunking employs a hierarchical approach that attempts to split text at natural boundaries while respecting size constraints. This method balances efficiency with semantic preservation.\n",
    "\n",
    "### Splitting Hierarchy\n",
    "The algorithm tries to split text in the following order of preference:\n",
    "1. **Paragraph breaks** (`\\n\\n`) - Preserves conceptual boundaries\n",
    "2. **Line breaks** (`\\n`) - Maintains sentence structure when possible\n",
    "3. **Spaces** (` `) - Avoids breaking words\n",
    "4. **Character-level** - Last resort when size constraints are strict\n",
    "\n",
    "### Characteristics\n",
    "- **Intelligence**: Respects natural text boundaries\n",
    "- **Flexibility**: Configurable separator hierarchy\n",
    "- **Balance**: Good trade-off between speed and quality\n",
    "- **Best Use Cases**: General-purpose text chunking for most applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure recursive character text splitter with hierarchical boundary detection\n",
    "# This approach intelligently chooses split points based on natural text structure\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=5000,                    # Target chunk size in characters\n",
    "    chunk_overlap=50,                   # Overlap for context continuity\n",
    "    separators=[\"\\n\\n\", \"\\n\"]           # Hierarchical split preferences: paragraphs, then lines\n",
    ")\n",
    "\n",
    "# Apply recursive chunking strategy to the sample document\n",
    "recursive_chunks = recursive_splitter.split_text(sample_data)\n",
    "\n",
    "# Display results summary\n",
    "print(\"RECURSIVE CHUNKING RESULTS\")\n",
    "print(f\"Number of chunks: {len(recursive_chunks)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Show first chunk demonstrating boundary-aware splitting\n",
    "print(recursive_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 3: Unstructured Document-Aware Chunking\n",
    "\n",
    "### Overview\n",
    "The Unstructured library provides sophisticated document parsing that recognizes and preserves document structure. This approach understands document elements like titles, headers, paragraphs, lists, and tables before applying chunking logic.\n",
    "\n",
    "### Document Understanding Capabilities\n",
    "- **Element Detection**: Automatically identifies titles, headers, body text, captions\n",
    "- **Structure Preservation**: Maintains hierarchical relationships between elements\n",
    "- **Content Classification**: Distinguishes between different types of content\n",
    "- **Intelligent Grouping**: Chunks content based on semantic coherence rather than arbitrary size\n",
    "\n",
    "### Characteristics\n",
    "- **Accuracy**: High-quality structure recognition\n",
    "- **Semantic Preservation**: Maintains document logic and flow\n",
    "- **Flexibility**: Handles diverse document formats and layouts\n",
    "- **Best Use Cases**: Documents with clear structure, academic papers, reports, books\n",
    "\n",
    "### Implementation Details\n",
    "We'll process the PDF directly to extract structural elements before applying title-based chunking:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse PDF with structure recognition using Unstructured library\n",
    "# Fast strategy balances speed with reasonable accuracy for document element detection\n",
    "elements = partition_pdf(\n",
    "    pdf_path, \n",
    "    strategy=\"fast\",                    \n",
    "    infer_table_structure=True          # Attempt to preserve table structure\n",
    ")\n",
    "\n",
    "# Apply title-based intelligent chunking that respects document hierarchy\n",
    "# This groups content under relevant headings and maintains semantic coherence\n",
    "unstructured_chunks = chunk_by_title(\n",
    "    elements, \n",
    "    max_characters=5000                 # Maximum chunk size while preserving structure\n",
    ")\n",
    "\n",
    "# Convert chunk objects to text strings for analysis (limiting for demo purposes)\n",
    "unstructured_chunks = [str(chunk) for chunk in unstructured_chunks[:6]]\n",
    "\n",
    "# Display results summary\n",
    "print(\"UNSTRUCTURED CHUNKING RESULTS\")\n",
    "print(f\"Number of chunks: {len(unstructured_chunks)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Show first chunk demonstrating structure-aware processing\n",
    "print(unstructured_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 4: Docling Advanced Hybrid Chunking\n",
    "\n",
    "### Overview\n",
    "\n",
    "Docling Hybrid Chunking processes documents by combining structured PDF parsing, markdown conversion, and intelligent chunking. \n",
    "It preserves hierarchical structure while creating semantically meaningful chunks.\n",
    "\n",
    "### Processing Pipeline\n",
    "\n",
    "- **PDF Conversion** ‚Äì Converts PDF documents to structured markdown.\n",
    "- **Header-Aware Splitting** ‚Äì Uses markdown headers (#, ##) to guide chunk boundaries while preserving headers for context.\n",
    "- **Hybrid Chunking** ‚Äì Integrates token-based limits with semantic awareness for fine-grained and meaningful chunks.\n",
    "- **Document Loading** ‚Äì Processes documents through the Docling loader, applying the hybrid chunking pipeline.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Structure Preservation** ‚Äì Maintains headings and document hierarchy.\n",
    "- **Semantic Awareness** ‚Äì Considers content meaning alongside structure.\n",
    "- **Custom Token Limits** ‚Äì Fine-grained chunks optimized for downstream embedding models.\n",
    "- **Advanced Parsing** ‚Äì Handles complex PDF layouts converted to markdown.\n",
    "\n",
    "### Characteristics\n",
    "\n",
    "- **Accuracy** ‚Äì Produces high-quality chunks that respect structure and content.\n",
    "- **Completeness** ‚Äì Preserves key information across document sections.\n",
    "- **Resource Usage** ‚Äì More computationally intensive due to hybrid processing.\n",
    "- **Best Use Cases** ‚Äì Complex or high-value documents where structure and context matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure markdown header-based splitter for structured document processing\n",
    "# This respects the hierarchical structure created by Docling's markdown conversion\n",
    "splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[\n",
    "        (\"#\", \"Header_1\"),              # Top-level headers (titles, major sections)\n",
    "        (\"##\", \"Header_2\"),             # Second-level headers (subsections)\n",
    "    ],\n",
    "    strip_headers=False                 # Preserve headers in chunks for context\n",
    ")\n",
    "\n",
    "# Initialize Docling loader with advanced hybrid chunking capabilities\n",
    "# Combines PDF parsing, markdown conversion, and intelligent chunking\n",
    "loader = DoclingLoader(\n",
    "    file_path=pdf_path,\n",
    "    export_type=ExportType.MARKDOWN,   # Convert to structured markdown format\n",
    "    chunker=HybridChunker(              # Advanced semantic-syntactic chunking\n",
    "        tokenizer=EMBEDDING_MODEL,      # Use same tokenizer as embedding model\n",
    "        max_tokens=100                  # Conservative token limit for fine-grained chunks\n",
    "    )\n",
    ")\n",
    "\n",
    "# Process document through Docling pipeline\n",
    "docs = loader.load()\n",
    "\n",
    "# Apply header-aware splitting to the markdown-converted content\n",
    "# Creates chunks that respect document structure and semantic boundaries\n",
    "docling_chunks = [\n",
    "    split.page_content \n",
    "    for doc in docs \n",
    "    for split in splitter.split_text(doc.page_content)\n",
    "]\n",
    "\n",
    "# Display results summary\n",
    "print(\"DOCLING CHUNKING RESULTS\")\n",
    "print(f\"Number of chunks: {len(docling_chunks)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Show first chunk demonstrating advanced structure preservation\n",
    "print(docling_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Strategy Comparative Analysis\n",
    "\n",
    "This section provides a quantitative comparison of the four chunking strategies implemented above. We'll examine key metrics including chunk count, average chunk length, and qualitative characteristics to understand the trade-offs between different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define strategy comparison dataset with descriptive metadata\n",
    "strategies = [\n",
    "    (\"Baseline\", baseline_chunks, \"Character-based splitting\"),\n",
    "    (\"Recursive\", recursive_chunks, \"Boundary-aware splitting\"), \n",
    "    (\"Unstructured\", unstructured_chunks, \"Structure-aware parsing\"),\n",
    "    (\"Docling\", docling_chunks, \"Advanced hybrid parsing\")\n",
    "]\n",
    "\n",
    "# Calculate comparative metrics for each chunking strategy\n",
    "comparison_data = []\n",
    "for name, chunks, description in strategies:\n",
    "    # Compute average chunk length as primary size metric\n",
    "    avg_length = sum(len(chunk) for chunk in chunks) / len(chunks)\n",
    "    \n",
    "    # Compile strategy performance summary\n",
    "    comparison_data.append({\n",
    "        \"Strategy\": name,\n",
    "        \"Chunks\": len(chunks),              # Total number of chunks produced\n",
    "        \"Avg Length\": f\"{avg_length:.0f}\",  # Mean characters per chunk\n",
    "        \"Description\": description          # Strategy characterization\n",
    "    })\n",
    "\n",
    "# Create comparison DataFrame for structured analysis\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Display formatted comparison results\n",
    "print(\"CHUNKING STRATEGY COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del baseline_chunks, recursive_chunks, unstructured_chunks, docling_chunks\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Database Integration and Collection Management\n",
    "\n",
    "This section demonstrates connecting to Qdrant, a high-performance vector database, to explore existing chunk collections. Vector databases are essential for RAG systems as they enable semantic search and similarity-based retrieval of document chunks.\n",
    "\n",
    "## Qdrant Vector Database\n",
    "Qdrant provides:\n",
    "- **High Performance**: Optimized for similarity search at scale\n",
    "- **Flexibility**: Support for various distance metrics and filtering\n",
    "- **Scalability**: Handles large collections efficiently\n",
    "- **Integration**: Seamless integration with embedding models\n",
    "\n",
    "## Collection Exploration\n",
    "We'll examine existing collections to understand how different chunking strategies have been previously processed and stored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not QDRANT_URL or not QDRANT_KEY:\n",
    "    print(\"Warning: Qdrant credentials not found. Please set QDRANT_URL and QDRANT_KEY\")\n",
    "    qdrant_client = None\n",
    "else:\n",
    "    # Initialize Qdrant client with cloud credentials\n",
    "    qdrant_client = QdrantClient(\n",
    "        url=QDRANT_URL,    # Cloud instance endpoint\n",
    "        api_key=QDRANT_KEY # Authentication key\n",
    "    )\n",
    "    print(\"Qdrant connection established\")\n",
    "    \n",
    "    # Retrieve and display available vector collections\n",
    "    # Each collection typically represents a different chunking strategy or dataset\n",
    "    collections = qdrant_client.get_collections()\n",
    "    print(\"\\nAVAILABLE QDRANT COLLECTIONS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if collections.collections:\n",
    "        # List all collections with their basic information\n",
    "        for collection in collections.collections:\n",
    "            print(f\"Collection: {collection.name}\")\n",
    "            # Additional collection metadata could be displayed here\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No collections found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scroll_iter = qdrant_client.scroll(\n",
    "    collection_name=\"baseline\",\n",
    "    limit=2          \n",
    ")\n",
    "\n",
    "points, next_page = scroll_iter\n",
    "for p in points:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Evaluation Framework\n",
    "\n",
    "### **What We're Evaluating**\n",
    "We measure **end-to-end RAG pipeline performance** by testing how well each chunking strategy enables:\n",
    "- **Information Retrieval**: Finding relevant document chunks\n",
    "- **Answer Generation**: Producing accurate, helpful responses\n",
    "- **Source Fidelity**: Maintaining truthfulness to original documents\n",
    "\n",
    "---\n",
    "\n",
    "### üìè **Core Evaluation Metrics**\n",
    "\n",
    "| Metric | **What It Measures** | **Key Question** |\n",
    "|--------|---------------------|------------------|\n",
    "| **Answer Relevancy** | How well the generated answer addresses the user's question | *\"Does this answer actually help the user?\"* |\n",
    "| **Faithfulness** | Whether the answer is grounded in the retrieved context | *\"Is the answer truthful to the source material?\"* |\n",
    "| **Context Precision** | Quality and relevance of retrieved document chunks | *\"Did we retrieve the right information?\"* |\n",
    "| **Context Recall** | Completeness of information retrieval | *\"Did we get all the necessary information?\"* |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load standardized test queries for consistent evaluation across all chunking strategies\n",
    "# These queries are designed to test different aspects of retrieval and comprehension\n",
    "import json\n",
    "with open(\"../tests/test_queries.json\", 'r') as f:\n",
    "    test_queries = json.load(f)\n",
    "\n",
    "# Display sample query to demonstrate evaluation approach\n",
    "print(\"\\nSample Test Query:\")\n",
    "print(f\"Question: {test_queries[0]['user_input']}\")\n",
    "print(f\"Source Document: {test_queries[0]['source_file']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize comprehensive chunking evaluation framework\n",
    "# This evaluator will test all four chunking strategies against standardized queries\n",
    "evaluator = ChunkingEvaluator(\n",
    "    query_count=2,  # Number of test queries per strategy\n",
    "    metrics=[       # RAGAS evaluation metrics for comprehensive assessment\n",
    "        'answer_relevancy',    \n",
    "        'faithfulness',        \n",
    "        'context_precision',   \n",
    "        'context_recall'       \n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Starting comprehensive chunking evaluation...\")\n",
    "print(\"This process evaluates all strategies against standardized queries using RAGAS metrics.\")\n",
    "print(\"Expected duration: 3-5 minutes depending on system performance.\\n\")\n",
    "\n",
    "try:\n",
    "    # Execute evaluation across all chunking strategies\n",
    "    # This runs complete RAG pipelines for each strategy with identical test conditions\n",
    "    results_df = evaluator.evaluate_all_strategies()\n",
    "    print(\"\\nEvaluation completed successfully!\")\n",
    "    \n",
    "    # Provide summary of evaluation results\n",
    "    print(f\"Results matrix: {results_df.shape}\")\n",
    "    print(\"Analysis includes performance across all metrics and strategies\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Evaluation failed with error: {e}\")\n",
    "    print(\"Check vector database connectivity and API credentials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display formatted results\n",
    "evaluator.print_results(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydata_tutorial_test",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
