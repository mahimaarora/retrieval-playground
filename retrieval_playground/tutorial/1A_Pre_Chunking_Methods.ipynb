{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Retrieval Methods: Comprehensive Guide to Document Chunking Strategies\n",
    "\n",
    "This notebook provides a comprehensive comparison of document chunking strategies for Retrieval-Augmented Generation (RAG) systems. Document chunking is a critical preprocessing step that directly impacts the quality of information retrieval and subsequent answer generation.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "### Four Core Document Chunking Strategies\n",
    "1. **Baseline Chunking**: Character-based splitting with fixed size limits\n",
    "2. **Recursive Character Chunking**: Hierarchical text splitting that respects natural language boundaries\n",
    "3. **Unstructured Chunking**: Structure-aware document processing that preserves semantic elements\n",
    "4. **Docling Chunking**: Advanced hybrid parsing with sophisticated document understanding\n",
    "\n",
    "### Evaluation Framework\n",
    "- Systematic evaluation methodology for chunking strategies\n",
    "- Key performance metrics for RAG system assessment\n",
    "- Evidence-based recommendations for strategy selection\n",
    "- Practical trade-offs between quality, speed, and complexity\n",
    "\n",
    "## Dataset and Methodology\n",
    "\n",
    "We demonstrate these techniques using research papers from the [mahimaarora025/research_papers](https://huggingface.co/datasets/mahimaarora025/research_papers/tree/main/sample_research_papers) dataset. This dataset contains peer-reviewed academic papers spanning multiple domains:\n",
    "- Analytics and Data Science\n",
    "- Computer Vision\n",
    "- Generative AI\n",
    "- Machine Learning\n",
    "- Statistics\n",
    "\n",
    "The diverse academic content provides an excellent testbed for evaluating chunking strategies across different document structures and content types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup and Dependencies\n",
    "\n",
    "This section initializes the required libraries and configures the environment for our chunking strategy comparison. We'll use a combination of document processing libraries, language models, and evaluation frameworks.\n",
    "\n",
    "### Key Dependencies\n",
    "- **LangChain**: Document loading and text splitting utilities\n",
    "- **Unstructured**: Advanced document parsing and structure recognition\n",
    "- **Docling**: State-of-the-art document conversion and chunking\n",
    "- **HuggingFace Transformers**: Embedding models for semantic similarity\n",
    "- **Qdrant**: Vector database for storing and retrieving document chunks\n",
    "- **RAGAS**: Evaluation framework for RAG system assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports completed\n"
     ]
    }
   ],
   "source": [
    "# Core Python libraries for file handling and data manipulation\n",
    "import os\n",
    "import tempfile\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "# Dataset loading from HuggingFace Hub\n",
    "from datasets import load_dataset\n",
    "\n",
    "# LangChain ecosystem for document processing and language model integration\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Advanced document processing libraries\n",
    "from unstructured.partition.pdf import partition_pdf  # PDF parsing with structure recognition\n",
    "from unstructured.chunking.title import chunk_by_title  # Title-based intelligent chunking\n",
    "from docling.document_converter import DocumentConverter  # Advanced document conversion\n",
    "from docling.chunking import HybridChunker  # Hybrid semantic-syntactic chunking\n",
    "\n",
    "# Vector database and embeddings for semantic search\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "# Data analysis and evaluation frameworks\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from retrieval_playground.src.pre_retrieval.chunking_evaluation import ChunkingEvaluator\n",
    "\n",
    "# Additional Docling components for markdown processing\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_docling.loader import ExportType\n",
    "from langchain_docling import DoclingLoader\n",
    "\n",
    "# Environment variable management\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# System configuration and API keys\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")  # Gemini API for language model operations\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")  # Qdrant cloud instance URL\n",
    "QDRANT_KEY = os.getenv(\"QDRANT_KEY\")  # Qdrant authentication key\n",
    "EMBEDDING_MODEL = \"Qwen/Qwen3-Embedding-0.6B\"  # Lightweight multilingual embedding model\n",
    "\n",
    "print(\"Imports completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 14:15:55,075 - INFO - Use pytorch device_name: mps\n",
      "2025-09-22 14:15:55,076 - INFO - Load pretrained SentenceTransformer: Qwen/Qwen3-Embedding-0.6B\n",
      "2025-09-22 14:15:58,229 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models initialized\n"
     ]
    }
   ],
   "source": [
    "# Verify API credentials are available\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"Please set GOOGLE_API_KEY environment variable\")\n",
    "\n",
    "# Initialize Google Gemini language model for text generation and evaluation\n",
    "# Using Gemini 2.0 Flash for fast inference with low temperature for consistency\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.1,  # Low temperature for deterministic outputs\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "# Initialize HuggingFace embedding model for semantic similarity computation\n",
    "# Qwen3-Embedding-0.6B provides efficient multilingual embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL\n",
    ")\n",
    "\n",
    "print(\"Models initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loading and Preprocessing\n",
    "\n",
    "This section demonstrates loading a research paper to serve as our test document for comparing chunking strategies. We'll use a representative academic paper that contains typical document structures found in research literature.\n",
    "\n",
    "### Data Source\n",
    "The test document comes from the research papers collection available at:\n",
    "https://huggingface.co/datasets/mahimaarora025/research_papers/tree/main/sample_research_papers\n",
    "\n",
    "### Document Characteristics\n",
    "Academic papers provide excellent test cases for chunking strategies because they contain:\n",
    "- Abstract and introduction sections\n",
    "- Multiple hierarchical headings\n",
    "- Mathematical formulations and equations\n",
    "- References and citations\n",
    "- Tables and figures with captions\n",
    "- Mixed text densities and complexity levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"../data/sample_research_papers/Generative_AI_2025_Frozen_in_Time__Parameter-Efficient_Time_Series_Transformers_via___Reservoir-Ind.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Initialize PDF loader for the selected research paper\n",
    "loader = PyPDFLoader(str(pdf_path))\n",
    "pdf_docs = loader.load()  # Load all pages as separate document objects\n",
    "\n",
    "# Extract text content from all pages\n",
    "sample_data = []\n",
    "for i in range(len(pdf_docs)):\n",
    "    sample_data.append(pdf_docs[i].page_content)\n",
    "\n",
    "# Combine all pages into a single text document with page breaks preserved\n",
    "# Using double newlines to maintain natural document flow for chunking\n",
    "sample_data = '\\n\\n'.join(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Chunking Strategy Implementations\n",
    "\n",
    "This section provides hands-on demonstrations of four distinct chunking approaches. Each strategy represents a different philosophy for dividing documents into manageable pieces while preserving semantic coherence and structural integrity.\n",
    "\n",
    "## Strategy 1: Baseline Character-Based Chunking\n",
    "\n",
    "### Overview\n",
    "The baseline approach uses simple character counting to divide text into fixed-size chunks. This method prioritizes speed and simplicity over semantic preservation.\n",
    "\n",
    "### Characteristics\n",
    "- **Speed**: Fastest execution time\n",
    "- **Simplicity**: Minimal configuration required\n",
    "- **Limitations**: May split sentences, paragraphs, or concepts mid-way\n",
    "- **Best Use Cases**: Large-scale processing where speed trumps precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 14:15:58,643 - WARNING - Created a chunk of size 5502, which is longer than the specified 5000\n",
      "2025-09-22 14:15:58,643 - WARNING - Created a chunk of size 6621, which is longer than the specified 5000\n",
      "2025-09-22 14:15:58,643 - WARNING - Created a chunk of size 5268, which is longer than the specified 5000\n",
      "2025-09-22 14:15:58,643 - WARNING - Created a chunk of size 6493, which is longer than the specified 5000\n",
      "2025-09-22 14:15:58,644 - WARNING - Created a chunk of size 5953, which is longer than the specified 5000\n",
      "2025-09-22 14:15:58,644 - WARNING - Created a chunk of size 5565, which is longer than the specified 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE CHUNKING RESULTS\n",
      "Number of chunks: 8\n",
      "--------------------------------------------------\n",
      "Frozen in Time: Parameter-Efficient Time Series\n",
      "Transformers via Reservoir-Induced Feature Expansion\n",
      "and Fixed Random Dynamics\n",
      "Pradeep Singha,*, Mehak Sharmaa, Anupriya Deya and Balasubramanian Ramana\n",
      "aMachine Intelligence Lab, Department of Computer Science and Engineering, IIT Roorkee, Roorkee-247667, India\n",
      "ORCID (Pradeep Singh): https://orcid.org/0000-0002-5372-3355, ORCID (Mehak Sharma):\n",
      "https://orcid.org/0009-0001-3102-1045, ORCID (Anupriya Dey): https://orcid.org/0009-0000-1630-1017, ORCID\n",
      "(Balasubramanian Raman): https://orcid.org/0000-0001-6277-6267\n",
      "Abstract. Transformers are the de-facto choice for sequence mod-\n",
      "elling, yet their quadratic self-attention and weak temporal bias can\n",
      "make long-range forecasting both expensive and brittle. We intro-\n",
      "duce FreezeTST, a lightweight hybrid that interleavesfrozen random-\n",
      "feature (reservoir) blocks with standard trainable Transformer lay-\n",
      "ers. The frozen blocks endow the network with rich nonlinear mem-\n",
      "ory at no optimisation cost; the trainable layers learn to query this\n",
      "memory through self-attention. The design cuts trainable parame-\n",
      "ters and also lowers wall-clock training time, while leaving infer-\n",
      "ence complexity unchanged. On seven standard long-term forecast-\n",
      "ing benchmarks, FreezeTST consistently matches or surpasses spe-\n",
      "cialised variants such as Informer, Autoformer, and PatchTST; with\n",
      "substantially lower compute. Our results show that embedding reser-\n",
      "voir principles within Transformers offers a simple, principled route\n",
      "to efficient long-term time-series prediction.\n",
      "1 Introduction\n",
      "Forecasting the future evolution of high-dimensional time series\n",
      "underpins safety-critical tasks such as renewable-grid dispatch, in-\n",
      "traday portfolio re-balancing, urban congestion mitigation, clini-\n",
      "cal decision-support and early warning of epidemiological surges\n",
      "[17, 7, 21, 36, 34]. What makes these problems hard is the simultane-\n",
      "ous presence of (i) long-range dependencies that may span hundreds\n",
      "of steps, (ii) strong seasonality and abrupt regime shifts, and (iii)\n",
      "training sets that are small relative to the combinatorial space of tem-\n",
      "poral patterns. Transformer encoders have emerged as a promising\n",
      "remedy because self-attention provides a content-adaptive alternative\n",
      "to the fixed convolution or recurrent receptive fields of earlier models\n",
      "[30]. Yet two structural flaws limit their effectiveness when horizons\n",
      "stretch into the hundreds: the O(T2) memory and time complex-\n",
      "ity of full attention, and the fact that positional encodings merely\n",
      "tag rather than enforce chronology, so permutation-invariant heads\n",
      "can still blur causal order. Empirically, even carefully engineered\n",
      "variants—Informer with ProbSparse attention [37], Autoformer with\n",
      "auto-correlation blocks [33], FEDformer with Fourier filters [38],\n",
      "Pyraformer with pyramidal multi-resolution attention [19], and Log-\n",
      "Trans with log-sparse attention [18]—fail to dominate across the\n",
      "∗ Corresponding Author. Email: pradeep.cs@sric.iitr.ac.in\n",
      "Long-Sequence Time-Series Forecasting (LSTF) benchmark; a re-\n",
      "cent work by Zeng et al. shows several cases where a one-layer lin-\n",
      "ear extrapolator wins outright [35]. These observations signal that\n",
      "further architectural principles, not just attention accelerators, are re-\n",
      "quired.\n",
      "Reservoir computing offers a complementary principle. By fixing\n",
      "the weights of a large nonlinear dynamical system and training only\n",
      "a linear read-out, echo-state networks (ESNs) turn temporal credit\n",
      "assignment into a convex, single-step regression problem while re-\n",
      "taining universal approximation power in the limit of infinite width\n",
      "[15, 16, 20]. The cost is a design trade-off: a spectral radius close\n",
      "to unity grants long memory but risks numerical instability, whereas\n",
      "heavy damping stabilises the dynamics at the price of premature for-\n",
      "getting. Recent work has begun to fuse these ideas with attention.\n",
      "Shen et al. froze alternate layers of a BERT encoder and observed\n",
      "comparable accuracy on language benchmarks at half the training\n",
      "cost [28]. Their results suggest that random, untrained transforma-\n",
      "tions can act as useful priors rather than noise—raising an open ques-\n",
      "tion for forecasting:\n",
      "Can a Transformer for time-series forecasting inherit the\n",
      "memory capacity of an echo-state reservoir, and if so, how\n",
      "should one combine the sequential bias of a reservoir with the\n",
      "pattern-matching flexibility of self-attention so that each com-\n",
      "pensates for the other’s weaknesses?\n",
      "We answer in the affirmative with a hybrid Reservoir-Transformer\n",
      "that interposes a frozen, randomly initialised block between atten-\n",
      "tion layers. The reservoir continually integrates incoming patches,\n",
      "providing a stable state vector that preserves sub-sequence statistics\n",
      "far beyond the Transformer’s sliding window, while attention learns\n",
      "to query this state adaptively. Because the recurrent/frozen weights\n",
      "are never updated, the model’s trainable parameter count and mem-\n",
      "ory footprint are roughly halved, yet its receptive field extends well\n",
      "past the H = 96 −720-step horizons used in the LSTF suite. Ex-\n",
      "tensive experiments on ETTh/ETTm, Weather, Electricity and ILI\n",
      "data [23, 37] show that our model matches or exceeds the strongest\n",
      "published baselines, including PatchTST and the best linear meth-\n",
      "ods, with up to 22% shorter training time. A supporting theoretical\n",
      "analysis (§3.2) proves that (i) the alternating frozen/trainable stack\n",
      "is non-expansive, ensuring gradient stability, and (ii) the reservoir’s\n",
      "arXiv:2508.18130v1  [cs.LG]  25 Aug 2025\n"
     ]
    }
   ],
   "source": [
    "# Configure baseline character-based text splitter\n",
    "# This approach splits text based on character count with minimal intelligence\n",
    "baseline_splitter = CharacterTextSplitter(\n",
    "    chunk_size=5000,        # Target chunk size in characters\n",
    "    chunk_overlap=100,      # Character overlap between consecutive chunks\n",
    "    separator=\"\\n\\n\"        # Preferred split point (paragraph breaks)\n",
    ")\n",
    "\n",
    "# Apply baseline chunking to the sample document\n",
    "baseline_chunks = baseline_splitter.split_text(sample_data)\n",
    "\n",
    "# Display results summary\n",
    "print(\"BASELINE CHUNKING RESULTS\")\n",
    "print(f\"Number of chunks: {len(baseline_chunks)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Show first chunk as example\n",
    "print(baseline_chunks[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 2: Recursive Character Chunking\n",
    "\n",
    "### Overview\n",
    "Recursive character chunking employs a hierarchical approach that attempts to split text at natural boundaries while respecting size constraints. This method balances efficiency with semantic preservation.\n",
    "\n",
    "### Splitting Hierarchy\n",
    "The algorithm tries to split text in the following order of preference:\n",
    "1. **Paragraph breaks** (`\\n\\n`) - Preserves conceptual boundaries\n",
    "2. **Line breaks** (`\\n`) - Maintains sentence structure when possible\n",
    "3. **Spaces** (` `) - Avoids breaking words\n",
    "4. **Character-level** - Last resort when size constraints are strict\n",
    "\n",
    "### Characteristics\n",
    "- **Intelligence**: Respects natural text boundaries\n",
    "- **Flexibility**: Configurable separator hierarchy\n",
    "- **Balance**: Good trade-off between speed and quality\n",
    "- **Best Use Cases**: General-purpose text chunking for most applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECURSIVE CHUNKING RESULTS\n",
      "Number of chunks: 15\n",
      "--------------------------------------------------\n",
      "Frozen in Time: Parameter-Efficient Time Series\n",
      "Transformers via Reservoir-Induced Feature Expansion\n",
      "and Fixed Random Dynamics\n",
      "Pradeep Singha,*, Mehak Sharmaa, Anupriya Deya and Balasubramanian Ramana\n",
      "aMachine Intelligence Lab, Department of Computer Science and Engineering, IIT Roorkee, Roorkee-247667, India\n",
      "ORCID (Pradeep Singh): https://orcid.org/0000-0002-5372-3355, ORCID (Mehak Sharma):\n",
      "https://orcid.org/0009-0001-3102-1045, ORCID (Anupriya Dey): https://orcid.org/0009-0000-1630-1017, ORCID\n",
      "(Balasubramanian Raman): https://orcid.org/0000-0001-6277-6267\n",
      "Abstract. Transformers are the de-facto choice for sequence mod-\n",
      "elling, yet their quadratic self-attention and weak temporal bias can\n",
      "make long-range forecasting both expensive and brittle. We intro-\n",
      "duce FreezeTST, a lightweight hybrid that interleavesfrozen random-\n",
      "feature (reservoir) blocks with standard trainable Transformer lay-\n",
      "ers. The frozen blocks endow the network with rich nonlinear mem-\n",
      "ory at no optimisation cost; the trainable layers learn to query this\n",
      "memory through self-attention. The design cuts trainable parame-\n",
      "ters and also lowers wall-clock training time, while leaving infer-\n",
      "ence complexity unchanged. On seven standard long-term forecast-\n",
      "ing benchmarks, FreezeTST consistently matches or surpasses spe-\n",
      "cialised variants such as Informer, Autoformer, and PatchTST; with\n",
      "substantially lower compute. Our results show that embedding reser-\n",
      "voir principles within Transformers offers a simple, principled route\n",
      "to efficient long-term time-series prediction.\n",
      "1 Introduction\n",
      "Forecasting the future evolution of high-dimensional time series\n",
      "underpins safety-critical tasks such as renewable-grid dispatch, in-\n",
      "traday portfolio re-balancing, urban congestion mitigation, clini-\n",
      "cal decision-support and early warning of epidemiological surges\n",
      "[17, 7, 21, 36, 34]. What makes these problems hard is the simultane-\n",
      "ous presence of (i) long-range dependencies that may span hundreds\n",
      "of steps, (ii) strong seasonality and abrupt regime shifts, and (iii)\n",
      "training sets that are small relative to the combinatorial space of tem-\n",
      "poral patterns. Transformer encoders have emerged as a promising\n",
      "remedy because self-attention provides a content-adaptive alternative\n",
      "to the fixed convolution or recurrent receptive fields of earlier models\n",
      "[30]. Yet two structural flaws limit their effectiveness when horizons\n",
      "stretch into the hundreds: the O(T2) memory and time complex-\n",
      "ity of full attention, and the fact that positional encodings merely\n",
      "tag rather than enforce chronology, so permutation-invariant heads\n",
      "can still blur causal order. Empirically, even carefully engineered\n",
      "variants—Informer with ProbSparse attention [37], Autoformer with\n",
      "auto-correlation blocks [33], FEDformer with Fourier filters [38],\n",
      "Pyraformer with pyramidal multi-resolution attention [19], and Log-\n",
      "Trans with log-sparse attention [18]—fail to dominate across the\n",
      "∗ Corresponding Author. Email: pradeep.cs@sric.iitr.ac.in\n",
      "Long-Sequence Time-Series Forecasting (LSTF) benchmark; a re-\n",
      "cent work by Zeng et al. shows several cases where a one-layer lin-\n",
      "ear extrapolator wins outright [35]. These observations signal that\n",
      "further architectural principles, not just attention accelerators, are re-\n",
      "quired.\n",
      "Reservoir computing offers a complementary principle. By fixing\n",
      "the weights of a large nonlinear dynamical system and training only\n",
      "a linear read-out, echo-state networks (ESNs) turn temporal credit\n",
      "assignment into a convex, single-step regression problem while re-\n",
      "taining universal approximation power in the limit of infinite width\n",
      "[15, 16, 20]. The cost is a design trade-off: a spectral radius close\n",
      "to unity grants long memory but risks numerical instability, whereas\n",
      "heavy damping stabilises the dynamics at the price of premature for-\n",
      "getting. Recent work has begun to fuse these ideas with attention.\n",
      "Shen et al. froze alternate layers of a BERT encoder and observed\n",
      "comparable accuracy on language benchmarks at half the training\n",
      "cost [28]. Their results suggest that random, untrained transforma-\n",
      "tions can act as useful priors rather than noise—raising an open ques-\n",
      "tion for forecasting:\n",
      "Can a Transformer for time-series forecasting inherit the\n",
      "memory capacity of an echo-state reservoir, and if so, how\n",
      "should one combine the sequential bias of a reservoir with the\n",
      "pattern-matching flexibility of self-attention so that each com-\n",
      "pensates for the other’s weaknesses?\n",
      "We answer in the affirmative with a hybrid Reservoir-Transformer\n",
      "that interposes a frozen, randomly initialised block between atten-\n",
      "tion layers. The reservoir continually integrates incoming patches,\n",
      "providing a stable state vector that preserves sub-sequence statistics\n",
      "far beyond the Transformer’s sliding window, while attention learns\n",
      "to query this state adaptively. Because the recurrent/frozen weights\n",
      "are never updated, the model’s trainable parameter count and mem-\n",
      "ory footprint are roughly halved, yet its receptive field extends well\n"
     ]
    }
   ],
   "source": [
    "# Configure recursive character text splitter with hierarchical boundary detection\n",
    "# This approach intelligently chooses split points based on natural text structure\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=5000,                    # Target chunk size in characters\n",
    "    chunk_overlap=50,                   # Overlap for context continuity\n",
    "    separators=[\"\\n\\n\", \"\\n\"]           # Hierarchical split preferences: paragraphs, then lines\n",
    ")\n",
    "\n",
    "# Apply recursive chunking strategy to the sample document\n",
    "recursive_chunks = recursive_splitter.split_text(sample_data)\n",
    "\n",
    "# Display results summary\n",
    "print(\"RECURSIVE CHUNKING RESULTS\")\n",
    "print(f\"Number of chunks: {len(recursive_chunks)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Show first chunk demonstrating boundary-aware splitting\n",
    "print(recursive_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 3: Unstructured Document-Aware Chunking\n",
    "\n",
    "### Overview\n",
    "The Unstructured library provides sophisticated document parsing that recognizes and preserves document structure. This approach understands document elements like titles, headers, paragraphs, lists, and tables before applying chunking logic.\n",
    "\n",
    "### Document Understanding Capabilities\n",
    "- **Element Detection**: Automatically identifies titles, headers, body text, captions\n",
    "- **Structure Preservation**: Maintains hierarchical relationships between elements\n",
    "- **Content Classification**: Distinguishes between different types of content\n",
    "- **Intelligent Grouping**: Chunks content based on semantic coherence rather than arbitrary size\n",
    "\n",
    "### Characteristics\n",
    "- **Accuracy**: High-quality structure recognition\n",
    "- **Semantic Preservation**: Maintains document logic and flow\n",
    "- **Flexibility**: Handles diverse document formats and layouts\n",
    "- **Best Use Cases**: Documents with clear structure, academic papers, reports, books\n",
    "\n",
    "### Implementation Details\n",
    "We'll process the PDF directly to extract structural elements before applying title-based chunking:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 14:15:58,650 - INFO - PDF text extraction failed, skip text extraction...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "UNSTRUCTURED CHUNKING RESULTS\n",
      "Number of chunks: 0\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Show first chunk demonstrating structure-aware processing\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43munstructured_chunks\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Parse PDF with structure recognition using Unstructured library\n",
    "# Fast strategy balances speed with reasonable accuracy for document element detection\n",
    "elements = partition_pdf(\n",
    "    pdf_path, \n",
    "    strategy=\"fast\",                    \n",
    "    infer_table_structure=True          # Attempt to preserve table structure\n",
    ")\n",
    "\n",
    "# Apply title-based intelligent chunking that respects document hierarchy\n",
    "# This groups content under relevant headings and maintains semantic coherence\n",
    "unstructured_chunks = chunk_by_title(\n",
    "    elements, \n",
    "    max_characters=5000                 # Maximum chunk size while preserving structure\n",
    ")\n",
    "\n",
    "# Convert chunk objects to text strings for analysis (limiting for demo purposes)\n",
    "unstructured_chunks = [str(chunk) for chunk in unstructured_chunks[:6]]\n",
    "\n",
    "# Display results summary\n",
    "print(\"UNSTRUCTURED CHUNKING RESULTS\")\n",
    "print(f\"Number of chunks: {len(unstructured_chunks)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Show first chunk demonstrating structure-aware processing\n",
    "print(unstructured_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 4: Docling Advanced Hybrid Chunking\n",
    "\n",
    "### Overview\n",
    "Docling represents the current state-of-the-art in document processing, offering advanced PDF parsing with conversion to structured markdown format. The hybrid chunker combines semantic understanding with syntactic rules for optimal chunk boundaries.\n",
    "\n",
    "### Advanced Processing Pipeline\n",
    "1. **Document Conversion**: PDF to structured markdown with layout preservation\n",
    "2. **Element Recognition**: Advanced AI-powered detection of document components\n",
    "3. **Hybrid Chunking**: Combines token-level and semantic-level chunking strategies\n",
    "4. **Header-Aware Splitting**: Respects markdown headers for natural boundaries\n",
    "\n",
    "### Key Innovations\n",
    "- **AI-Powered Parsing**: Uses machine learning models for superior accuracy\n",
    "- **Layout Understanding**: Preserves spatial relationships and document flow\n",
    "- **Multi-Modal Processing**: Handles text, images, tables, and complex layouts\n",
    "- **Semantic Chunking**: Considers content meaning in addition to structure\n",
    "\n",
    "### Characteristics\n",
    "- **Accuracy**: Highest quality document understanding\n",
    "- **Completeness**: Preserves maximum document information\n",
    "- **Computational Cost**: Most resource-intensive approach\n",
    "- **Best Use Cases**: High-value documents, complex layouts, maximum quality requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure markdown header-based splitter for structured document processing\n",
    "# This respects the hierarchical structure created by Docling's markdown conversion\n",
    "splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[\n",
    "        (\"#\", \"Header_1\"),              # Top-level headers (titles, major sections)\n",
    "        (\"##\", \"Header_2\"),             # Second-level headers (subsections)\n",
    "    ],\n",
    "    strip_headers=False                 # Preserve headers in chunks for context\n",
    ")\n",
    "\n",
    "# Initialize Docling loader with advanced hybrid chunking capabilities\n",
    "# Combines PDF parsing, markdown conversion, and intelligent chunking\n",
    "loader = DoclingLoader(\n",
    "    file_path=pdf_path,\n",
    "    export_type=ExportType.MARKDOWN,   # Convert to structured markdown format\n",
    "    chunker=HybridChunker(              # Advanced semantic-syntactic chunking\n",
    "        tokenizer=EMBEDDING_MODEL,      # Use same tokenizer as embedding model\n",
    "        max_tokens=100                  # Conservative token limit for fine-grained chunks\n",
    "    )\n",
    ")\n",
    "\n",
    "# Process document through Docling pipeline\n",
    "docs = loader.load()\n",
    "\n",
    "# Apply header-aware splitting to the markdown-converted content\n",
    "# Creates chunks that respect document structure and semantic boundaries\n",
    "docling_chunks = [\n",
    "    split.page_content \n",
    "    for doc in docs \n",
    "    for split in splitter.split_text(doc.page_content)\n",
    "]\n",
    "\n",
    "# Display results summary\n",
    "print(\"DOCLING CHUNKING RESULTS\")\n",
    "print(f\"Number of chunks: {len(docling_chunks)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Show first chunk demonstrating advanced structure preservation\n",
    "print(docling_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Strategy Comparative Analysis\n",
    "\n",
    "This section provides a quantitative comparison of the four chunking strategies implemented above. We'll examine key metrics including chunk count, average chunk length, and qualitative characteristics to understand the trade-offs between different approaches.\n",
    "\n",
    "## Comparative Metrics\n",
    "\n",
    "The analysis focuses on several key dimensions:\n",
    "- **Chunk Count**: Total number of chunks generated\n",
    "- **Average Length**: Mean character count per chunk\n",
    "- **Consistency**: Variance in chunk sizes\n",
    "- **Boundary Quality**: How well chunks respect semantic boundaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define strategy comparison dataset with descriptive metadata\n",
    "strategies = [\n",
    "    (\"Baseline\", baseline_chunks, \"Character-based splitting\"),\n",
    "    (\"Recursive\", recursive_chunks, \"Boundary-aware splitting\"), \n",
    "    (\"Unstructured\", unstructured_chunks, \"Structure-aware parsing\"),\n",
    "    (\"Docling\", docling_chunks, \"Advanced hybrid parsing\")\n",
    "]\n",
    "\n",
    "# Calculate comparative metrics for each chunking strategy\n",
    "comparison_data = []\n",
    "for name, chunks, description in strategies:\n",
    "    # Compute average chunk length as primary size metric\n",
    "    avg_length = sum(len(chunk) for chunk in chunks) / len(chunks)\n",
    "    \n",
    "    # Compile strategy performance summary\n",
    "    comparison_data.append({\n",
    "        \"Strategy\": name,\n",
    "        \"Chunks\": len(chunks),              # Total number of chunks produced\n",
    "        \"Avg Length\": f\"{avg_length:.0f}\",  # Mean characters per chunk\n",
    "        \"Description\": description          # Strategy characterization\n",
    "    })\n",
    "\n",
    "# Create comparison DataFrame for structured analysis\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Display formatted comparison results\n",
    "print(\"CHUNKING STRATEGY COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Database Integration and Collection Management\n",
    "\n",
    "This section demonstrates connecting to Qdrant, a high-performance vector database, to explore existing chunk collections. Vector databases are essential for RAG systems as they enable semantic search and similarity-based retrieval of document chunks.\n",
    "\n",
    "## Qdrant Vector Database\n",
    "Qdrant provides:\n",
    "- **High Performance**: Optimized for similarity search at scale\n",
    "- **Flexibility**: Support for various distance metrics and filtering\n",
    "- **Scalability**: Handles large collections efficiently\n",
    "- **Integration**: Seamless integration with embedding models\n",
    "\n",
    "## Collection Exploration\n",
    "We'll examine existing collections to understand how different chunking strategies have been previously processed and stored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection to Qdrant vector database\n",
    "# Check for required credentials before attempting connection\n",
    "if not QDRANT_URL or not QDRANT_KEY:\n",
    "    print(\"Warning: Qdrant credentials not found. Please set QDRANT_URL and QDRANT_KEY\")\n",
    "    qdrant_client = None\n",
    "else:\n",
    "    # Initialize Qdrant client with cloud credentials\n",
    "    qdrant_client = QdrantClient(\n",
    "        url=QDRANT_URL,    # Cloud instance endpoint\n",
    "        api_key=QDRANT_KEY # Authentication key\n",
    "    )\n",
    "    print(\"Qdrant connection established\")\n",
    "    \n",
    "    # Retrieve and display available vector collections\n",
    "    # Each collection typically represents a different chunking strategy or dataset\n",
    "    collections = qdrant_client.get_collections()\n",
    "    print(\"\\nAVAILABLE QDRANT COLLECTIONS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if collections.collections:\n",
    "        # List all collections with their basic information\n",
    "        for collection in collections.collections:\n",
    "            print(f\"Collection: {collection.name}\")\n",
    "            # Additional collection metadata could be displayed here\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No collections found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scroll_iter = qdrant_client.scroll(\n",
    "    collection_name=\"baseline\",\n",
    "    limit=2          \n",
    ")\n",
    "\n",
    "points, next_page = scroll_iter\n",
    "for p in points:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Evaluation Framework\n",
    "\n",
    "This section implements a rigorous evaluation methodology to assess the real-world performance of different chunking strategies. We'll use a complete RAG pipeline with standardized test queries to measure retrieval quality and answer generation effectiveness.\n",
    "\n",
    "## Evaluation Methodology\n",
    "\n",
    "### Test Framework\n",
    "- **RAGAS Metrics**: Industry-standard RAG evaluation framework\n",
    "- **Standardized Queries**: Consistent test questions across all strategies\n",
    "- **Controlled Variables**: Same embedding model, LLM, and retrieval parameters\n",
    "- **Multiple Dimensions**: Retrieval quality, answer relevance, faithfulness, precision, recall\n",
    "\n",
    "### Key Performance Indicators\n",
    "1. **Answer Relevancy**: How well generated answers address the question\n",
    "2. **Faithfulness**: Accuracy and consistency with source material\n",
    "3. **Context Precision**: Quality of retrieved chunks for answering\n",
    "4. **Context Recall**: Completeness of information retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load standardized test queries for consistent evaluation across all chunking strategies\n",
    "# These queries are designed to test different aspects of retrieval and comprehension\n",
    "import json\n",
    "with open(\"../retrieval_playground/tests/test_queries.json\", 'r') as f:\n",
    "    test_queries = json.load(f)\n",
    "\n",
    "# Display sample query to demonstrate evaluation approach\n",
    "print(\"\\nSample Test Query:\")\n",
    "print(f\"Question: {test_queries[0]['user_input']}\")\n",
    "print(f\"Source Document: {test_queries[0]['source_file']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize comprehensive chunking evaluation framework\n",
    "# This evaluator will test all four chunking strategies against standardized queries\n",
    "evaluator = ChunkingEvaluator(\n",
    "    query_count=2,  # Number of test queries per strategy\n",
    "    metrics=[       # RAGAS evaluation metrics for comprehensive assessment\n",
    "        'answer_relevancy',    \n",
    "        'faithfulness',        \n",
    "        'context_precision',   \n",
    "        'context_recall'       \n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Starting comprehensive chunking evaluation...\")\n",
    "print(\"This process evaluates all strategies against standardized queries using RAGAS metrics.\")\n",
    "print(\"Expected duration: 3-5 minutes depending on system performance.\\n\")\n",
    "\n",
    "try:\n",
    "    # Execute evaluation across all chunking strategies\n",
    "    # This runs complete RAG pipelines for each strategy with identical test conditions\n",
    "    results_df = evaluator.evaluate_all_strategies()\n",
    "    print(\"\\nEvaluation completed successfully!\")\n",
    "    \n",
    "    # Provide summary of evaluation results\n",
    "    print(f\"Results matrix: {results_df.shape}\")\n",
    "    print(\"Analysis includes performance across all metrics and strategies\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Evaluation failed with error: {e}\")\n",
    "    print(\"Check vector database connectivity and API credentials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display formatted results\n",
    "evaluator.print_results(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
